var documenterSearchIndex = {"docs":
[{"location":"QN.html#QN","page":"QN","title":"QN","text":"","category":"section"},{"location":"QN.html#Description","page":"QN","title":"Description","text":"","category":"section"},{"location":"QN.html#ITensors.QuantumNumbers.QN","page":"QN","title":"ITensors.QuantumNumbers.QN","text":"A QN object stores a collection of up to four named values such as (\"Sz\",1) or (\"N\",0). These values can include a third integer \"m\" which makes them obey addition modulo m, for example (\"P\",1,2) for a value obeying addition mod 2. (The default is regular integer addition).\n\nAdding or subtracting pairs of QN objects performs addition and subtraction element-wise on each of the named values. If a name is missing from the collection, its value is treated as zero.\n\n\n\n\n\n","category":"type"},{"location":"QN.html#Constructors","page":"QN","title":"Constructors","text":"","category":"section"},{"location":"QN.html#ITensors.QuantumNumbers.QN-Tuple","page":"QN","title":"ITensors.QuantumNumbers.QN","text":"QN(qvs...)\n\nConstruct a QN from a set of up to four named value tuples.\n\nExamples\n\nq = QN((\"Sz\",1))\nq = QN((\"N\",1),(\"Sz\",-1))\nq = QN((\"P\",0,2),(\"Sz\",0)).\n\n\n\n\n\n","category":"method"},{"location":"QN.html#ITensors.QuantumNumbers.QN-2","page":"QN","title":"ITensors.QuantumNumbers.QN","text":"QN(name,val::Int,modulus::Int=1)\n\nConstruct a QN with a single named value by providing the name, value, and optional modulus.\n\n\n\n\n\n","category":"type"},{"location":"QN.html#ITensors.QuantumNumbers.QN-3","page":"QN","title":"ITensors.QuantumNumbers.QN","text":"QN(val::Int,modulus::Int=1)\n\nConstruct a QN with a single unnamed value (equivalent to the name being the empty string) with optional modulus.\n\n\n\n\n\n","category":"type"},{"location":"QN.html#Properties","page":"QN","title":"Properties","text":"","category":"section"},{"location":"QN.html#ITensors.val-Tuple{QN, Any}","page":"QN","title":"ITensors.val","text":"val(q::QN,name)\n\nGet the value within the QN q corresponding to the string name\n\n\n\n\n\n","category":"method"},{"location":"QN.html#ITensors.QuantumNumbers.modulus-Tuple{QN, Any}","page":"QN","title":"ITensors.QuantumNumbers.modulus","text":"modulus(q::QN,name)\n\nGet the modulus within the QN q corresponding to the string name\n\n\n\n\n\n","category":"method"},{"location":"QN.html#Related-Functions","page":"QN","title":"Related Functions","text":"","category":"section"},{"location":"QN.html#Base.zero-Tuple{QN}","page":"QN","title":"Base.zero","text":"zero(q::QN)\n\nReturns a QN object containing the same names as q, but with all values set to zero.\n\n\n\n\n\n","category":"method"},{"location":"HDF5FileFormats.html#HDF5-File-Formats","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"","category":"section"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"This page lists the formats for the HDF5 representations of various types in the ITensors module.","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"HDF5 is a portable file format which has a directory structure similar to a file system. In addition to containing \"groups\" (= directories) and \"datasets\" (= files), groups can have \"attributes\" appended to them, which are similar to 'tags' or 'keywords'. Unless otherwise specified, integers are 64 bit and are signed (H5T_STD_I64LE) unless explicitly stated. (For example, the \"id\" field of the Index type is stored as an unsigned 64 bit integer (H5T_STD_U64LE).)","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Each type in ITensor which is writeable to HDF5 is written to its own group, with the name of the group either specified by the user or specified to some default value when it is a subgroup of another ITensor type (for example, the Index type saves its TagSet in a subgroup named \"tags\").","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Each group corresponding to an ITensors type always carries the following attributes:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"type\" –- a string such as Index or TagSet specifying the information necessary to determine the type of the object saved to the HDF5 group\n\"version\" –- an integer specifying the file format version used to store the data. This version is in general different from the release version of ITensors.jl. The purpose of the version number is to aid in maintaining backwards compatibility, while allowing the format to be occasionally changed.","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"The C++ version of ITensor uses exactly the same file formats listed below, for the purpose of interoperability with the Julia version of ITensor, even though conventions such as the \"type\" field values are Julia-centric.","category":"page"},{"location":"HDF5FileFormats.html#tagset_hdf5","page":"HDF5 File Formats","title":"TagSet","text":"","category":"section"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"HDF5 file format for the ITensors.TagSet type.","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Attributes:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"version\" = 1\n\"type\" = \"TagSet\"","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Datasets and Subgroups:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"tags\" [string] = a comma separated string of the tags in the TagSet","category":"page"},{"location":"HDF5FileFormats.html#qn_hdf5","page":"HDF5 File Formats","title":"QN","text":"","category":"section"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"HDF5 file format for the ITensors.QN type.","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Attributes:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"version\" = 1\n\"type\" = \"QN\"","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Datasets and Subgroups:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"names\" [group] = array of strings (length 4) of names of quantum numbers\n\"vals\" [group] = array of integers (length 4) of quantum number values\n\"mods\" [group] = array of integers (length 4) of moduli of quantum numbers","category":"page"},{"location":"HDF5FileFormats.html#qnblocks_hdf5","page":"HDF5 File Formats","title":"QNBlocks","text":"","category":"section"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"HDF5 file format for the ITensors.QNBlocks type. (Note: QNBlocks is equivalent to Vector{Pair{QN, Int64}}.)","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Attributes:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"version\" = 1\n\"type\" = \"QNBlocks\"","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Datasets and Subgroups:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"length\" [integer] = the number of blocks (length of Vector)\n\"dims\" [group] = array of (integer) dimensions of each block\n\"QN[n]\" [group] = these groups \"QN[1]\", \"QN[2]\", etc. correspond to the QN of each block","category":"page"},{"location":"HDF5FileFormats.html#index_hdf5","page":"HDF5 File Formats","title":"Index","text":"","category":"section"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"HDF5 file format for the ITensors.Index type.","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Attributes:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"version\" = 1\n\"type\" = \"Index\"\n\"space_type\" = \"Int\" if the Index is a regular, dense Index or \"QNBlocks\" if the Index is a QNIndex (carries QN subspace information)","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Datasets and Subgroups:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"id\" [unsigned integer] = id number of the Index\n\"dim\" [integer] = dimension of the Index\n\"dir\" [integer] = arrow direction of the Index, +1 for ITensors.Out and -1 for ITensors.In\n\"plev\" [integer] = prime level of the Index\n\"tags\" [group] = the TagSet of the Index","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Optional Datasets and Subgroups:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"space\" [group] = if the \"space_type\" attribute is \"QNBlocks\", this group is present and represents a QNBlocks object","category":"page"},{"location":"HDF5FileFormats.html#indexset_hdf5","page":"HDF5 File Formats","title":"IndexSet","text":"","category":"section"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"HDF5 file format for types in the Union type ITensors.Indices which includes IndexSet and tuples of Index objects.","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Attributes:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"version\" = 1\n\"type\" = \"IndexSet\"","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Datasets and Subgroups:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"length\" [integer] = number of indices\n\"index_n\" [group] = for n=1 to n=length each of these groups contains an Index","category":"page"},{"location":"HDF5FileFormats.html#itensor_hdf5","page":"HDF5 File Formats","title":"ITensor","text":"","category":"section"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"HDF5 file format for the ITensors.ITensor type.","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Attributes:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"version\" = 1\n\"type\" = \"ITensor\"","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Datasets and Subgroups:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"inds\" [group] = indices of the ITensor\n\"storage\" [group] = storage of the ITensor (note that some earlier versions of ITensors.jl may call this group \"store\")","category":"page"},{"location":"HDF5FileFormats.html#dense_hdf5","page":"HDF5 File Formats","title":"NDTensors.Dense","text":"","category":"section"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"HDF5 file format for objects which are subtypes of ITensors.NDTensors.Dense.","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Attributes:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"version\" = 1\n\"type\" = \"Dense{Float64}\" or \"Dense{ComplexF64}\"","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Datasets and Subgroups:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"data\" = array of either real or complex values (in the same dataset format used by the HDF5.jl library for storing Vector{Float64} or Vector{ComplexF64})","category":"page"},{"location":"HDF5FileFormats.html#blocksparse_hdf5","page":"HDF5 File Formats","title":"NDTensors.BlockSparse","text":"","category":"section"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"HDF5 file format for objects which are subtypes of ITensors.NDTensors.BlockSparse.","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Attributes:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"version\" = 1\n\"type\" = \"BlockSparse{Float64}\" or \"BlockSparse{ComplexF64}\"","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"Datasets and Subgroups:","category":"page"},{"location":"HDF5FileFormats.html","page":"HDF5 File Formats","title":"HDF5 File Formats","text":"\"ndims\" [integer] = number of dimensions (order) of the tensor\n\"offsets\" = block offset data flattened into an array of integers\n\"data\" = array of either real or complex values (in the same dataset format used by the HDF5.jl library for storing Vector{Float64} or Vector{ComplexF64})","category":"page"},{"location":"examples/ITensor.html#itensor_examples","page":"ITensor Examples","title":"ITensor Code Examples","text":"","category":"section"},{"location":"examples/ITensor.html#Print-Indices-of-an-ITensor","page":"ITensor Examples","title":"Print Indices of an ITensor","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Sometimes the printout of an ITensor can be rather large, whereas you might only want to see its indices. For these cases, just wrap the ITensor in the function inds like this:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"@show inds(T)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"or this","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"println(\"T inds = \",inds(T))","category":"page"},{"location":"examples/ITensor.html#Getting-and-Setting-Elements-of-an-ITensor","page":"ITensor Examples","title":"Getting and Setting Elements of an ITensor","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Say we have an ITensor constructed as:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"i = Index(3,\"index_i\")\nj = Index(2,\"index_j\")\nk = Index(4,\"index_k\")\n\nT = ITensor(i,j,k)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"An ITensor constructed this way starts with all of its elements equal to zero. (Technically it allocates no storage at all but this is an implementation detail.)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Setting Elements","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"To set an element of this ITensor, such as the element where (i,j,k) = (2,1,3), you can do the following:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"T[i=>2,j=>1,k=>3] = -3.2","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"In the Julia language, the notation a=>b is a built-in notation for making a Pair(a,b) object.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Because the Index objects are passed to T along with their values, passing them in a different order has exactly the same effect:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"# Both of these lines of code do the same thing:\nT[j=>1,i=>2,k=>3] = -3.2\nT[j=>1,k=>3,i=>2] = -3.2","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Getting Elements","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"You can retrieve individual elements of an ITensor by accessing them through the same notation used to set elements:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"el = T[j=>1,i=>2,k=>3]\nprintln(\"The (i,j,k) = (2,1,3) element of T is \",el)","category":"page"},{"location":"examples/ITensor.html#Making-ITensors-from-Arrays","page":"ITensor Examples","title":"Making ITensors from Arrays","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"To initialize all of the elements of an ITensor at once, you can pass a Julia array into the ITensor constructor.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"For example, if we want to construct an ITensor A with indices i,j we can initialize it from a matrix as follows:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"M = [1.0 2.0;\n     3.0 4.0]\n\ni = Index(2,\"i\")\nj = Index(2,\"j\")\n\nA = ITensor(M,i,j)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"More generally we can use an nth-order (n-dimensional) Julia array to initialize an ITensor:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"T = randn(4,7,2)\n\nk = Index(4,\"index_k\")\nl = Index(7,\"index_l\")\nm = Index(2,\"index_m\")\n\nB = ITensor(T,k,l,m)","category":"page"},{"location":"examples/ITensor.html#Making-Arrays-from-ITensors","page":"ITensor Examples","title":"Making Arrays from ITensors","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Not only can we make an ITensor from a Julia array, but we can also convert an ITensor back into a Julia array.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Say we have made an ITensor with two indices:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"using ITensors # hide\nk = Index(4,\"index_k\")\nm = Index(2,\"index_m\")\n\nT = random_itensor(k,m)\n@show T\ndisplay(T) # hide","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Here we used the random_itensor constructor to fill T with random elements but we could make an ITensor some other way too.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Now to convert T into a regular Julia array A, use the Array constructor and pass the indices of T in the order that you want:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"A = Array(T,k,m)\n@show A","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"The reason you have to pass the indices is that the ordering of ITensor indices is an implementation detail and not part of the user interface. So when leaving the ITensor system and converting to a regular array, you must say what ordering of the indices you want. Making the array as A = Array(T,m,k) would give the transpose of the array in the code above.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Note that for efficiency reasons, the array returned by the array function will sometimes be a view of the ITensor, such that changing an element of A would also change the corresponding element of T. This is not always the case though: for example if the indices are passed in a different order from how the internal ITensor storage is arranged, or if  T is a block-sparse ITensor, since the (not stored) zero blocks will need to be filled in.","category":"page"},{"location":"examples/ITensor.html#Arithmetic-With-ITensors","page":"ITensor Examples","title":"Arithmetic With ITensors","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"ITensors can be added and subtracted and multiplied by scalars just like plain tensors can. But ITensors have the additional feature that you can add and subtract them even if their indices are in a different order from each other, as long as they have the same collection of indices.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"For example, say we have ITensors A, B, and C:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"i = Index(3,\"i\")\nj = Index(2,\"j\")\nk = Index(4,\"k\")\n\nA = random_itensor(i,j,k)\nB = random_itensor(i,j,k)\nC = random_itensor(k,i,j)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Above we have initialized these ITensors to have random elements, just for the sake of this example.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"We can then add or subtract these ITensors","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"R1 = A + B\nR2 = A - B\nR3 = A + B - C","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"or do more complicated operations involving real and complex scalars too:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"R4 = 2.0*A - B + C/(1+1im)","category":"page"},{"location":"examples/ITensor.html#Elementwise-Operations-on-ITensors","page":"ITensor Examples","title":"Elementwise Operations on ITensors","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"[Note: currently elementwise operations are only defined for dense ITensors, not for block-sparse QN ITensors.]","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"ITensors support Julia broadcasting operations, making it quite easy to carry out element-wise operations on them in a very similar way as for regular Julia arrays. As a concrete example, consider the following ITensor initialized with random elements","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"i = Index(2,\"i\")\nj = Index(3,\"j\")\n\nA = random_itensor(i,j)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Here are some examples of basic element-wise operations we can do using Julia's dotted operator broadcasting syntax.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"# Multiply every element of `A` by 2.0:\nA .*= 2.0","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"# Add 1.5 to every element of A\nA .+= 1.5","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"The dotted notation works for functions too:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"# Replace every element in A by its absolute value:\nA .= abs.(A)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"# Replace every element in A by the number 1.0\nA .= one.(A)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"If have another ITensor B = ITensor(j,i), which has the same set of indices though possibly in a different order, then we can also do element-wise operations involving both ITensors:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"# Add elements of A and B element-wise\nA .= A .+ B\n# Add elements of A and B element-wise with coefficients included\nA .= (2.0 .* A) .+ (-3.0 .* B)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Last but not least, it is possible to make custom functions yourself and broadcast them across elements of ITensors:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"myf(x) = 1.0/(1.0+exp(-x))\nT .= myf.(T)","category":"page"},{"location":"examples/ITensor.html#Making-an-ITensor-with-a-Single-Non-Zero-Element","page":"ITensor Examples","title":"Making an ITensor with a Single Non-Zero Element","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"It is often useful to make ITensors with all elements zero except for a specific element that is equal to 1.0. Use cases can include making product-state quantum wavefunctions or contracting single-element ITensors with other ITensors to set their indices to a fixed value.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"To make such an ITensor, use the onehot function. Borrowing terminology from engineering, a \"one hot\" vector or tensor has a single element equal to 1.0 and the rest zero. (In previous versions of ITensor this function was called setelt.)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"The ITensor function onehot takes one or more Index-value Pairs such as i=>2 and j=>1 and returns an ITensor with a 1.0 in the location specified by the Index values:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"using ITensors # hide\ni = Index(2)\nO1 = onehot(i=>1)\nprintln(O1)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"using ITensors # hide\ni = Index(2) # hide\nO2 = onehot(i=>2)\nprintln(O2)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"using ITensors # hide\ni = Index(2) # hide\nj = Index(3)\nT = onehot(i=>2,j=>3)\nprintln(T)","category":"page"},{"location":"examples/ITensor.html#Tracing-an-ITensor","page":"ITensor Examples","title":"Tracing an ITensor","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"An important operation involving a single tensor is tracing out certain pairs of indices. Say we have an ITensor A with indices i,j,l:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"i = Index(4,\"i\")\nj = Index(3,\"j\")\nl = Index(4,\"l\")\n\nA = random_itensor(i,j,l)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"and we want to trace A by summing over the indices i and l locked together, in other words: sum_i A^iji.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"To do this in ITensor, we can use a delta tensor, which you can think of as an identity operator or more generally a Kronecker delta or \"hyper-edge\":","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"(Image: )","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Viewed as an array, a delta tensor has all diagonal elements equal to 1.0 and zero otherwise.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Now we can compute the trace by contracting A with the delta tensor:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"trA = A * delta(i,l)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"(Image: )","category":"page"},{"location":"examples/ITensor.html#Factoring-ITensors-(SVD,-QR,-etc.)","page":"ITensor Examples","title":"Factoring ITensors (SVD, QR, etc.)","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"The ITensor approach to tensor factorizations emphasizes the structure of the factorization, and does not require knowing the index ordering.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"ITensor offers various tensor factorizations, such as the singular value decomposition (SVD) and the QR factorization. These are extended to the case of tensors by treating some of the indices as the \"row\" indices and the rest of the indices as the \"column\" indices, reshaping the tensor into a matrix to carry out the factorization, then restoring the tensor structure at the end. All of these steps are done for you by the ITensor system as we will see below.","category":"page"},{"location":"examples/ITensor.html#Singular-Value-Decomposition","page":"ITensor Examples","title":"Singular Value Decomposition","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"The singular value decomposition (SVD) is a matrix factorization that is also extremely useful for general tensors.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"As a brief review, the SVD is a factorization of a matrix M into the product","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"M = U S V^dagger","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"with U and V having the property U^dagger U = 1 and V^dagger V = 1. The matrix S is diagonal and has real, non-negative entries known as the singular values, which are typically ordered from largest to smallest. The SVD is well-defined for any matrix, including rectangular matrices. It also leads to a controlled approximation, where the error due to discarding columns of U and V is small if the corresponding singular values discarded are small.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"To compute the SVD of an ITensor, you only need to specify which indices are (collectively) the \"row\" indices (thinking of the ITensor as a matrix), with the rest assumed to be the \"column\" indices.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Say we have an ITensor with indices i,j, and k","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"T = ITensor(i,j,k)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"and we want to treat i and k as the \"row\" indices for the purpose of the SVD.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"To perform this SVD, we can call the function svd as follows:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"U,S,V = svd(T,(i,k))","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Diagrammatically the SVD operation above looks like:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"(Image: )","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"The guarantee of the svd function is that the ITensor product U*S*V gives us back an ITensor identical to T:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"@show norm(U*S*V - T) # typical output: norm(U*S*V-T) = 1E-14","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Full working example:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"i = Index(3,\"i\")\nj = Index(4,\"j\")\nk = Index(5,\"k\")\n\nT = random_itensor(i,j,k)\n\nU,S,V = svd(T,(i,k))\n\n@show norm(U*S*V-T)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Truncated SVD","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"An important use of the SVD is approximating a higher-rank tensor by a product of lower-rank tensors whose indices range over only a modest set of values.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"To obtain an approximate SVD in ITensor, pass one or more of the following accuracy parameters as named arguments:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"cutoff –- real number epsilon. Discard the smallest singular values lambda_n such that the <i>truncation error</i> is less than epsilon: $ \\frac{\\sum_{n\\in\\text{discarded}} \\lambda^2_n}{\\sum_{n} \\lambda^2_n} < \\epsilon \\:. $ Using a cutoff allows the SVD algorithm to truncate as many states as possible while still ensuring a certain accuracy.\nmaxdim –- integer M. If the number of singular values exceeds M, only the largest M will be retained.\nmindim –- integer m. At least m singular values will be retained, even if some fall below the cutoff","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Let us revisit the example above, but also provide some of these accuracy parameters","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"i = Index(10,\"i\")\nj = Index(40,\"j\")\nk = Index(20,\"k\")\nT = random_itensor(i,j,k)\n\nU,S,V = svd(T,(i,k),cutoff=1E-2)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Note that we have also made the indices larger so that the truncation performed will be non-trivial. In the code above, we specified that a cutoff of epsilon=10^-2 be used. We can check that the resulting factorization is now approximate by computing the squared relative error:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"truncerr = (norm(U*S*V - T)/norm(T))^2\n@show truncerr\n# typical output: truncerr = 8.24E-03","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Note how the computed error is below the cutoff epsilon we requested.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Full working example including truncation:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"i = Index(10,\"i\");\nj = Index(40,\"j\");\nk = Index(20,\"k\");\n\nT = random_itensor(i,j,k)\n\nU,S,V = svd(T,(i,k),cutoff=1E-2)\n\n@show norm(U*S*V-T)\n@show (norm(U*S*V - T)/norm(T))^2","category":"page"},{"location":"examples/ITensor.html#QR-Factorization","page":"ITensor Examples","title":"QR Factorization","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Computing the QR factorization of an ITensor works in a similar way as for the SVD. In addition to passing the ITensor you want to factorize, you must also pass the indices you want to end up on the tensor Q, in other words to be treated as the \"row\" indices for the purpose of defining the QR factorization.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Say we want to compute the QR factorization of an ITensor T with indices i,j,k, putting the indices i and k onto Q and the remaining indices onto R. We can do this as follows:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"(Image: )","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"T = random_itensor(i,j,k)\nQ,R = qr(T,(i,k);positive=true)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Note the use of the optional positive=true keyword argument, which ensures that the diagonal elements of R are non-negative. With this option, the QR factorization is unique, which can be useful in certain cases.","category":"page"},{"location":"examples/ITensor.html#Combining-Multiple-Indices-into-One-Index","page":"ITensor Examples","title":"Combining Multiple Indices into One Index","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"It can be very useful to combine or merge multiple indices of an ITensor into a single Index. Say we have an ITensor with indices i,j,k and we want to combine Index i and Index k into a new Index. This new Index (call it c) will have a dimension whose size is the dimension of i times the dimension of k.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"To carry out this procedure we can make a special kind of ITensor: a combiner. To make a combiner, call the function combiner, passing the indices you want to combine:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"using ITensors # hide\ni = Index(4,\"i\") # hide\nj = Index(3,\"j\") # hide\nk = Index(2,\"k\") # hide\nC = combiner(i,k; tags=\"c\")\nnothing # hide","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Then if we have an ITensor","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"T = random_itensor(i,j,k)\n@show inds(T)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"we can combine indices i and k by contracting with the combiner:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"CT = C * T\nnothing # hide","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Printing out the indices of the new ITensor CT we can see that it has only two indices:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"@show inds(CT)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"The first is the newly made combined Index, which was made for us by the combiner function and the second is the j Index of T which was not part of the combining process. To access the combined Index you can call the combinedind function on the combiner:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"ci = combinedind(C)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"We can visualize all of the steps above as follows: (Image: )","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Combining is not limited to two indices and you can combine any number of indices, in any order, using a combiner.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"To undo the combining process and uncombine the Index c back into i,k, just contract with the conjugate of the combiner ITensor dag(C).","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"UT = dag(C) * CT\n@show inds(UT)","category":"page"},{"location":"examples/ITensor.html#Write-and-Read-an-ITensor-to-Disk-with-HDF5","page":"ITensor Examples","title":"Write and Read an ITensor to Disk with HDF5","text":"","category":"section"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"info: Info\nMake sure to install the HDF5 package to use this feature. (Run julia> ] add HDF5 in the Julia REPL console.)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Saving ITensors to disk can be very useful. For example, you might encounter a bug in your own code, and by reading the ITensors involved from disk you can shortcut the process of running a lengthy algorithm over many times to reproduce the bug. Or you can save the output of an expensive calculation, such as a DMRG calculation, and use it as a starting point for multiple follow-up calculations such as computing time-dependent properties.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"ITensors can be written to files using the HDF5 format. HDF5 offers many benefits such as being portable across different machine types, and offers a standard interface across various libraries and languages.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Writing an ITensor to an HDF5 File","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Let's say you have an ITensor T which you have made or obtained from a calculation. To write it to an HDF5 file named \"myfile.h5\" you can use the following pattern:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"using HDF5\nf = h5open(\"myfile.h5\",\"w\")\nwrite(f,\"T\",T)\nclose(f)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Above, the string \"T\" can actually be any string you want such as \"ITensor T\" or \"Result Tensor\" and doesn't have to have the same name as the reference T. Closing the file f is optional and you can also write other objects to the same file before closing it.","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Reading an ITensor from an HDF5 File","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Say you have an HDF5 file \"myfile.h5\" which contains an ITensor stored as a dataset with the name \"T\". (Which would be the situation if you wrote it as in the example above.) To read this ITensor back from the HDF5 file, use the following pattern:","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"using HDF5\nf = h5open(\"myfile.h5\",\"r\")\nT = read(f,\"T\",ITensor)\nclose(f)","category":"page"},{"location":"examples/ITensor.html","page":"ITensor Examples","title":"ITensor Examples","text":"Note the ITensor argument to the read function, which tells Julia which read function to call and how to interpret the data stored in the HDF5 dataset named \"T\". In the future we might lift the requirement of providing the type and have it be detected automatically from the data stored in the file.","category":"page"},{"location":"getting_started/DebugChecks.html#Enabling-Debug-Checks","page":"Enabling Debug Checks","title":"Enabling Debug Checks","text":"","category":"section"},{"location":"getting_started/DebugChecks.html","page":"Enabling Debug Checks","title":"Enabling Debug Checks","text":"ITensor provides some optional checks for common errors, which we call \"debug checks\". These can be enabled with the command:","category":"page"},{"location":"getting_started/DebugChecks.html","page":"Enabling Debug Checks","title":"Enabling Debug Checks","text":"ITensors.enable_debug_checks()","category":"page"},{"location":"getting_started/DebugChecks.html","page":"Enabling Debug Checks","title":"Enabling Debug Checks","text":"and disabled with the command:","category":"page"},{"location":"getting_started/DebugChecks.html","page":"Enabling Debug Checks","title":"Enabling Debug Checks","text":"ITensors.disable_debug_checks()","category":"page"},{"location":"getting_started/DebugChecks.html","page":"Enabling Debug Checks","title":"Enabling Debug Checks","text":"We recommend enabling debug checks when you are developing and testing your code, and then disabling them when running in production to get the best performance.","category":"page"},{"location":"getting_started/DebugChecks.html","page":"Enabling Debug Checks","title":"Enabling Debug Checks","text":"For example, when debug checks are turned on, ITensor checks that all indices of an ITensor are unique (if they are not unique, it leads to undefined behavior in tensor operations like contraction, addition, and decomposition):","category":"page"},{"location":"getting_started/DebugChecks.html","page":"Enabling Debug Checks","title":"Enabling Debug Checks","text":"julia> using ITensors\n\njulia> i = Index(2)\n(dim=2|id=913)\n\njulia> A = random_itensor(i', i)\nITensor ord=2 (dim=2|id=913)' (dim=2|id=913)\nNDTensors.Dense{Float64, Vector{Float64}}\n\njulia> noprime(A)\nITensor ord=2 (dim=2|id=913) (dim=2|id=913)\nNDTensors.Dense{Float64, Vector{Float64}}\n\njulia> ITensors.enable_debug_checks()\nusing_debug_checks (generic function with 1 method)\n\njulia> noprime(A)\nERROR: Trying to create ITensors with collection of indices ((dim=2|id=913), (dim=2|id=913)). Indices must be unique.\nStacktrace:\n [1] error(s::String)\n   @ Base ./error.jl:33\n [2] macro expansion\n   @ ~/.julia/packages/ITensors/cu9Bo/src/itensor.jl:85 [inlined]\n [3] macro expansion\n   @ ~/.julia/packages/ITensors/cu9Bo/src/global_variables.jl:177 [inlined]\n [4] ITensor\n   @ ~/.julia/packages/ITensors/cu9Bo/src/itensor.jl:82 [inlined]\n [5] #itensor#123\n   @ ~/.julia/packages/ITensors/cu9Bo/src/itensor.jl:123 [inlined]\n [6] itensor(args::NDTensors.DenseTensor{Float64, 2, Tuple{Index{Int64}, Index{Int64}}, NDTensors.Dense{Float64, Vector{Float64}}})\n   @ ITensors ~/.julia/packages/ITensors/cu9Bo/src/itensor.jl:123\n [7] noprime(::ITensor; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})\n   @ ITensors ~/.julia/packages/ITensors/cu9Bo/src/itensor.jl:1211\n [8] noprime(::ITensor)\n   @ ITensors ~/.julia/packages/ITensors/cu9Bo/src/itensor.jl:1211\n [9] top-level scope\n   @ REPL[7]:1","category":"page"},{"location":"getting_started/DebugChecks.html","page":"Enabling Debug Checks","title":"Enabling Debug Checks","text":"You can track where debug checks are located in the code here, and add your own debug checks to your own code by wrapping your code with the macro ITensors.@debug_check.","category":"page"},{"location":"IndexType.html#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"IndexType.html#Description","page":"Index","title":"Description","text":"","category":"section"},{"location":"IndexType.html#ITensors.Index","page":"Index","title":"ITensors.Index","text":"An Index represents a single tensor index with fixed dimension dim. Copies of an Index compare equal unless their tags are different.\n\nAn Index carries a TagSet, a set of tags which are small strings that specify properties of the Index to help distinguish it from other Indices. There is a special tag which is referred to as the integer tag or prime level which can be incremented or decremented with special priming functions.\n\nInternally, an Index has a fixed id number, which is how the ITensor library knows two indices are copies of a single original Index. Index objects must have the same id, as well as the tags to compare equal.\n\n\n\n\n\n","category":"type"},{"location":"IndexType.html#ITensors.QNIndex","page":"Index","title":"ITensors.QNIndex","text":"A QN Index is an Index with QN block storage instead of just an integer dimension. The QN block storage is a vector of pairs of QNs and block dimensions. The total dimension of a QN Index is the sum of the dimensions of the blocks of the Index.\n\n\n\n\n\n","category":"type"},{"location":"IndexType.html#Constructors","page":"Index","title":"Constructors","text":"","category":"section"},{"location":"IndexType.html#ITensors.Index-Tuple{Int64}","page":"Index","title":"ITensors.Index","text":"Index(dim::Int; tags::Union{AbstractString, TagSet} = \"\",\n                plev::Int = 0)\n\nCreate an Index with a unique id, a TagSet given by tags, and a prime level plev.\n\nExamples\n\njulia> i = Index(2; tags=\"l\", plev=1)\n(dim=2|id=818|\"l\")'\n\njulia> dim(i)\n2\n\njulia> plev(i)\n1\n\njulia> tags(i)\n\"l\"\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.Index-Tuple{Int64, Union{ITensors.TagSets.GenericTagSet{BitIntegers.UInt256, 4}, AbstractString}}","page":"Index","title":"ITensors.Index","text":"Index(dim::Integer, tags::Union{AbstractString, TagSet}; plev::Int = 0)\n\nCreate an Index with a unique id and a tagset given by tags.\n\nExamples\n\njulia> i = Index(2, \"l,tag\")\n(dim=2|id=58|\"l,tag\")\n\njulia> dim(i)\n2\n\njulia> plev(i)\n0\n\njulia> tags(i)\n\"l,tag\"\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.Index-Tuple{Vararg{Pair{QN, Int64}}}","page":"Index","title":"ITensors.Index","text":"Index(qnblocks::Pair{QN, Int64}...; tags = \"\",\n                                    plev::Integer = 0)\n\nConstruct a QN Index from a list of pairs of QN and block dimensions.\n\nExample\n\nIndex(QN(\"Sz\", -1) => 1, QN(\"Sz\", 1) => 1; tags = \"i\")\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.Index-Tuple{Vector{Pair{QN, Int64}}}","page":"Index","title":"ITensors.Index","text":"Index(qnblocks::Vector{Pair{QN, Int64}}; tags = \"\", plev::Integer = 0)\n\nConstruct a QN Index from a Vector of pairs of QN and block dimensions.\n\nExample\n\nIndex([QN(\"Sz\", -1) => 1, QN(\"Sz\", 1) => 1]; tags = \"i\")\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.Index-Tuple{Vector{Pair{QN, Int64}}, Union{ITensors.TagSets.GenericTagSet{BitIntegers.UInt256, 4}, AbstractString}}","page":"Index","title":"ITensors.Index","text":"Index(qnblocks::Vector{Pair{QN, Int64}}, tags; plev::Integer = 0)\n\nConstruct a QN Index from a Vector of pairs of QN and block dimensions.\n\nExample\n\ni = Index([QN(\"Sz\", -1) => 1, QN(\"Sz\", 1) => 1], \"i\")\nidag = dag(i) # Same Index with arrow direction flipped\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#Properties","page":"Index","title":"Properties","text":"","category":"section"},{"location":"IndexType.html#ITensors.id-Tuple{Index}","page":"Index","title":"ITensors.id","text":"id(i::Index)\n\nObtain the id of an Index, which is a unique 64 digit integer.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.hasid-Tuple{Index, UInt64}","page":"Index","title":"ITensors.hasid","text":"hasid(i::Index, id::ITensors.IDType)\n\nCheck if an Index i has the provided id.\n\nExamples\n\njulia> i = Index(2)\n(dim=2|id=321)\n\njulia> hasid(i, id(i))\ntrue\n\njulia> j = Index(2)\n(dim=2|id=17)\n\njulia> hasid(i, id(j))\nfalse\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.tags-Tuple{Index}","page":"Index","title":"ITensors.tags","text":"tags(i::Index)\n\nObtain the TagSet of an Index.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.TagSets.set_strict_tags!-Tuple{Bool}","page":"Index","title":"ITensors.TagSets.set_strict_tags!","text":"set_strict_tags!(enable::Bool) -> Bool\n\n\nEnable or disable checking for overflow of the number of tags of a TagSet or the number of characters of a tag. If enabled (set to true), an error will be thrown if overflow occurs, otherwise the overflow will be ignored and the extra tags or tag characters will be dropped. This could cause unexpected bugs if tags are being used to distinguish Index objects that have the same ids and prime levels, but that is generally discouraged and should only be used if you know what you are doing.\n\nSee also ITensors.using_strict_tags.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.TagSets.using_strict_tags-Tuple{}","page":"Index","title":"ITensors.TagSets.using_strict_tags","text":"using_strict_tags() -> Bool\n\n\nSee if checking for overflow of the number of tags of a TagSet or the number of characters of a tag is enabled or disabled.\n\nSee also ITensors.set_strict_tags!.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.TagSets.hastags-Tuple{Index, Union{ITensors.TagSets.GenericTagSet{BitIntegers.UInt256, 4}, AbstractString}}","page":"Index","title":"ITensors.TagSets.hastags","text":"hastags(i::Index, ts::Union{AbstractString,TagSet})\n\nCheck if an Index i has the provided tags, which can be a string of comma-separated tags or a TagSet object.\n\nExamples\n\njulia> i = Index(2, \"SpinHalf,Site,n=3\")\n(dim=2|id=861|\"Site,SpinHalf,n=3\")\n\njulia> hastags(i, \"SpinHalf,Site\")\ntrue\n\njulia> hastags(i, \"Link\")\nfalse\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.plev-Tuple{Index}","page":"Index","title":"ITensors.plev","text":"plev(i::Index)\n\nObtain the prime level of an Index.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.hasplev-Tuple{Index, Int64}","page":"Index","title":"ITensors.hasplev","text":"hasplev(i::Index, plev::Int)\n\nCheck if an Index i has the provided prime level.\n\nExamples\n\njulia> i = Index(2; plev=2)\n(dim=2|id=543)''\n\njulia> hasplev(i, 2)\ntrue\n\njulia> hasplev(i, 1)\nfalse\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#NDTensors.dim-Tuple{Index}","page":"Index","title":"NDTensors.dim","text":"dim(i::Index)\n\nObtain the dimension of an Index.\n\nFor a QN Index, this is the sum of the block dimensions.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#Base.:==-Tuple{Index, Index}","page":"Index","title":"Base.:==","text":"==(i1::Index, i1::Index)\n\nCompare indices for equality. First the id's are compared, then the prime levels are compared, and finally the tags are compared.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.dir-Tuple{Index}","page":"Index","title":"ITensors.dir","text":"dir(i::Index)\n\nReturn the direction of an Index (ITensors.In, ITensors.Out, or ITensors.Neither).\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.hasqns-Tuple{Index}","page":"Index","title":"ITensors.hasqns","text":"hasqns(::Index)\n\nChecks of the Index has QNs or not.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#Priming-and-tagging-methods","page":"Index","title":"Priming and tagging methods","text":"","category":"section"},{"location":"IndexType.html#ITensors.prime-Tuple{Index, Int64}","page":"Index","title":"ITensors.prime","text":"prime(i::Index, plinc::Int = 1)\n\nReturn a copy of Index i with its prime level incremented by the amount plinc\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#Base.adjoint-Tuple{Index}","page":"Index","title":"Base.adjoint","text":"adjoint(i::Index)\n\nPrime an Index using the notation i'.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#Base.:^-Tuple{Index, Int64}","page":"Index","title":"Base.:^","text":"^(i::Index, pl::Int)\n\nPrime an Index using the notation i^3.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.setprime-Tuple{Index, Int64}","page":"Index","title":"ITensors.setprime","text":"setprime(i::Index, plev::Int)\n\nReturn a copy of Index i with its prime level set to plev\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.noprime-Tuple{Index}","page":"Index","title":"ITensors.noprime","text":"noprime(i::Index)\n\nReturn a copy of Index i with its prime level set to zero.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.settags-Tuple{Index, Any}","page":"Index","title":"ITensors.settags","text":"settags(i::Index, ts)\n\nReturn a copy of Index i with tags replaced by the ones given The ts argument can be a comma-separated string of tags or a TagSet.\n\nExamples\n\njulia> i = Index(2, \"SpinHalf,Site,n=3\")\n(dim=2|id=543|\"Site,SpinHalf,n=3\")\n\njulia> hastags(i, \"Link\")\nfalse\n\njulia> j = settags(i, \"Link,n=4\")\n(dim=2|id=543|\"Link,n=4\")\n\njulia> hastags(j, \"Link\")\ntrue\n\njulia> hastags(j, \"n=4,Link\")\ntrue\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.TagSets.addtags-Tuple{Index, Any}","page":"Index","title":"ITensors.TagSets.addtags","text":"addtags(i::Index,ts)\n\nReturn a copy of Index i with the specified tags added to the existing ones. The ts argument can be a comma-separated string of tags or a TagSet.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.TagSets.removetags-Tuple{Index, Any}","page":"Index","title":"ITensors.TagSets.removetags","text":"removetags(i::Index, ts)\n\nReturn a copy of Index i with the specified tags removed. The ts argument can be a comma-separated string of tags or a TagSet.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.TagSets.replacetags-Tuple{Index, Any, Any}","page":"Index","title":"ITensors.TagSets.replacetags","text":"replacetags(i::Index, tsold, tsnew)\n\nreplacetags(i::Index, tsold => tsnew)\n\nIf the tag set of i contains the tags specified by tsold, replaces these with the tags specified by tsnew, preserving any other tags. The arguments tsold and tsnew can be comma-separated strings of tags, or TagSet objects.\n\nExamples\n\njulia> i = Index(2; tags=\"l,x\", plev=1)\n(dim=2|id=83|\"l,x\")'\n\njulia> replacetags(i, \"l\", \"m\")\n(dim=2|id=83|\"m,x\")'\n\njulia> replacetags(i, \"l\" => \"m\")\n(dim=2|id=83|\"m,x\")'\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#Methods","page":"Index","title":"Methods","text":"","category":"section"},{"location":"IndexType.html#NDTensors.sim-Tuple{Index}","page":"Index","title":"NDTensors.sim","text":"sim(i::Index; tags = tags(i), plev = plev(i))\n\nProduces an Index with the same properties (dimension or QN structure) but with a new id.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.dag-Tuple{Index}","page":"Index","title":"ITensors.dag","text":"dag(i::Index)\n\nCopy an index i and reverse its direction.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.removeqns-Tuple{Index}","page":"Index","title":"ITensors.removeqns","text":"removeqns(::Index)\n\nRemoves the QNs from the Index, if it has any.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#Iterating","page":"Index","title":"Iterating","text":"","category":"section"},{"location":"IndexType.html#ITensors.eachval-Tuple{Index}","page":"Index","title":"ITensors.eachval","text":"eachval(i::Index)\n\nCreate an iterator whose values range over the dimension of the provided Index.\n\n\n\n\n\n","category":"method"},{"location":"IndexType.html#ITensors.eachindval-Tuple{Index}","page":"Index","title":"ITensors.eachindval","text":"eachindval(i::Index)\n\nCreate an iterator whose values are Pairs of the form i=>n with n from 1:dim(i). This iterator is useful for accessing elements of an ITensor in a loop without needing to know the ordering of the indices. See also eachindval(is::Index...).\n\n\n\n\n\n","category":"method"},{"location":"getting_started/Installing.html#Installing-Julia-and-ITensor","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"","category":"section"},{"location":"getting_started/Installing.html#Installing-Julia-Locally-and-On-a-Cluster","page":"Installing Julia and ITensor","title":"Installing Julia Locally and On a Cluster","text":"","category":"section"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Because Julia is a new language, it is usually not pre-installed on machines such as supercomputing clusters. But it is easy to install yourself both on your own machine and in your supercomputing environment. Here we will briefly cover installing Julia on your own machine, then discuss setting it up yourself on a supercomputer.","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Installing Julia on Your Own Machine","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"To install the Julia language, visit https://julialang.org/downloads/ for downloads and installation instructions. Or consider using your system's package manager.","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Cluster Install of Julia and ITensor","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"If you would like to use Julia on a remote cluster, such as at many labs or universities, but Julia is not available system-wide, you can still easily install your own local version of Julia. A local install will offer the same performance and features (package manager, etc.) as a system-wide install, and you can upgrade it at your own pace.","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Once you set up Julia in your cluster account, you can install ITensor in the same way as on your personal computer (see next section on installing ITensor).","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"To install Julia locally within your cluster account, follow these basic steps (details will vary depending on your setup):","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Download a binary version of Julia here. On a remote Unix or Linux cluster, you can use the program wget to download remote files. (Right click on the link on the Julia downloads page to the Generic Linux on x86, 64-bit Julia download to copy the link to pass to the wget program.)\nUse the tar program to uncompress the .tar.gz file you have downloaded.\nCreate a soft link somewhere in your PATH (such as in the bin/ subfolder of your home folder, which you might need to create) pointing to the file \"bin/julia\" inside of the uncompressed Julia folder you just created.","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"For example, the set of commands might look like this (where these commands are assumed to be executed in your home directory):","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"$ cd\n$ mkdir -p bin\n$ wget https://julialang-s3.julialang.org/bin/linux/x64/1.7/julia-1.7.2-linux-x86_64.tar.gz\n$ tar xvzf julia-1.7.2-linux-x86_64.tar.gz\n$ ln -s julia-1.7.2/bin/julia  bin/julia","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"If you want to install Julia 1.6.6, you would change 1.7 to 1.6 and 1.7.2 to 1.6.6. In general we recommend using the current stable release of Julia, which you can find out by going to the Julia Downloads page. We also don't recommend using versions of Julia below 1.6, which are no longer compatible with ITensors.jl as of ITensors 0.3.","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"After these steps, you should be able to type julia from your terminal to run Julia in interactive mode. If that works, then you have the Julia language and can run it in all the usual ways. If it does not work, you may need to log out and back in, and check that the bin directory is in your program execution path (PATH environment variable).","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Explanation of the sample commands above:","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"The first command cd goes to your home directory.\nThe second command makes a new folder bin/ under your home directory if it does not already exist.\nThe third command downloads the Julia language as a compressed tar.gz file. (You may want to do this step and the follwing steps in a different folder of your choosing.)\nThe fourth command uncompresses the tar.gz file into a folder called (in this example) julia-1.7.2.\nThe last command makes a soft link called julia in your bin directory which links to the Julia language binary within the folder you just unpacked containing the Julia language.","category":"page"},{"location":"getting_started/Installing.html#Installing-ITensor-(ITensors.jl-Package)","page":"Installing Julia and ITensor","title":"Installing ITensor (ITensors.jl Package)","text":"","category":"section"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Installing the Julia version of ITensor is easy once you have the Julia language installed. For more information about installing Julia, please see the Julia language downloads page.","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Once you have installed Julia on your machine,","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Enter the command julia to launch an interactive Julia session (a.k.a. the Julia \"REPL\")\nType ] to enter the package manager (pkg> prompt should now show)\nEnter the command add ITensors\nAfter installation completes, press backspace to return to the normal julia> prompt\nOptional but Recommended: Enter the command julia> using ITensors; ITensors.compile() to compile a large fraction of the ITensor library code and following the instructions afterward to make an alias for loading a pre-built ITensor system image with Julia. This step can take up to 10 minutes to complete but only has to be done once for each version of ITensor. See the section on compiling ITensor for more information.","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"Sample screenshot:","category":"page"},{"location":"getting_started/Installing.html","page":"Installing Julia and ITensor","title":"Installing Julia and ITensor","text":"(Image: )","category":"page"},{"location":"getting_started/RunningCodes.html#Running-ITensor-and-Julia-Codes","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"","category":"section"},{"location":"getting_started/RunningCodes.html#Basic-Example-Code-Template","page":"Running ITensor and Julia Codes","title":"Basic Example Code Template","text":"","category":"section"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"The basic outline of a code which uses the ITensor library is as follows","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"using ITensors\n\nlet\n  # ... your own code goes here ...\n  # For example:\n  i = Index(2,\"i\")\n  j = Index(3,\"j\")\n  T = random_itensor(i,j)\n  @show T\nend","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"The reason we recommend the let...end block is that code written in the Julia global scope can have some surprising behaviors. Putting your code into a let block avoids these issues.","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"Alternatively, you can wrap your code in a function:","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"using ITensors\n\nfunction main(; d1 = 2, d2 = 3)\n  # ... your own code goes here ...\n  # For example:\n  i = Index(d1,\"i\")\n  j = Index(d2,\"j\")\n  T = random_itensor(i,j)\n  @show T\nend\n\nmain(; d1 = 4, d2 = 5)","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"which can be useful in interactive mode, particularly if you might want to run your code with a variety of different arguments.","category":"page"},{"location":"getting_started/RunningCodes.html#Running-a-Script","page":"Running ITensor and Julia Codes","title":"Running a Script","text":"","category":"section"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"Now say you put the above code into a file named code.jl. Then you can run this code on the command line as follows","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"$ julia code.jl","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"This script-like mode of running Julia is convenient for running longer jobs, such as on a cluster.","category":"page"},{"location":"getting_started/RunningCodes.html#Running-Interactively","page":"Running ITensor and Julia Codes","title":"Running Interactively","text":"","category":"section"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"However, sometimes you want to do rapid development when first writing and testing a code. For this kind of work, the long startup and compilation times currently incurred by the Julia compiler can be a nuisance. Fortunately a nice solution is to alternate between modifying your code then running it by loading it into an already running Julia session.","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"To set up this kind of session, take the following steps:","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"Enter the interactive mode of Julia, by inputting the command julia on the command line. You will now be in the Julia \"REPL\" (read-eval-print loop) with the prompt julia> on the left of your screen.\nTo run a code such as the code.jl file discussed above, input the command\njulia> include(\"code.jl\")\nNote that you must be in the same folder as code.jl for this to work; otherwise input the entire path to the code.jl file. The code will run and you will see its output in the REPL.\nNow say you want to modify and re-run the code. To do this, just edit the file in an editor in another window, without closing your Julia session. Now run the command\njulia> include(\"code.jl\")\nagain and your updated code will run, but this time skipping any of the precompilation overhead incurred on previous steps.","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"The above steps to running a code interactively has a big advantage that you only have to pay the startup time of compiling ITensor and other libraries you are using once. Further changes to your code only incur very small extra compilation times, facilitating rapid development.","category":"page"},{"location":"getting_started/RunningCodes.html#Compiling-an-ITensor-System-Image","page":"Running ITensor and Julia Codes","title":"Compiling an ITensor System Image","text":"","category":"section"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"The above strategy of running code in the Julia REPL (interactive mode) works well, but still incurs a large start-up penalty for the first run of your code. Fortunately there is a nice way around this issue too: compiling ITensors.jl and making a system image built by the PackageCompiler.jl library.","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"To use this approach, we have provided a convenient one-line command:","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"julia> using ITensors, ITensorMPS, PackageCompiler\n\njulia> ITensors.compile()","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"Note that you need to have ITensors.jl, ITensorMPS.jl, and PackageCompiler.jl installed. It can take a few minutes to run, but you only have to run it once for a given version of ITensors.jl. When it is done, it will create a file sys_itensors.so in the directory ~/.julia/sysimages/.","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"To use the compiled system image together with Julia, run the julia command (for interactive mode or scripts) in the following way:","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"$ julia --sysimage ~/.julia/sysimages/sys_itensors.so","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"A convenient thing to do is to make an alias in your shell for this command. To do this, edit your .bashrc or .zshrc or similar file for the shell you use by adding the following line:","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"alias julia_itensors=\"julia --sysimage ~/.julia/sysimages/sys_itensors.so -e \\\"using ITensors\\\" -i \"","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"where of course you can use the command name you like when defining the alias. Now running commands like julia_itensors code.jl or julia_itensors to start an interactive session will have the ITensor system image pre-loaded and you will notice significantly faster startup times. The arguments -e \\\"using ITensors\\\" -i make it so that running julia_itensors also loads the ITensor library as soon as Julia starts up, so that you don't have to type using ITensors every time.","category":"page"},{"location":"getting_started/RunningCodes.html#Using-a-Compiled-Sysimage-in-Jupyter-or-VS-Code","page":"Running ITensor and Julia Codes","title":"Using a Compiled Sysimage in Jupyter or VS Code","text":"","category":"section"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"If you have compiled a sysimage for ITensor as shown above, you can use it in Jupyter by running the following code:","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"using IJulia\ninstallkernel(\"julia_ITensors\",\"--sysimage=~/.julia/sysimages/sys_itensors.so\")","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"in the Julia REPL (Julia console).","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"To load the ITensor sysimage in VS Code, you can add","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"\"--sysimage ~/.julia/sysimages/sys_itensors.so\"","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"as an argument under the julia.additionalArgs setting in your Settings.json file.","category":"page"},{"location":"getting_started/RunningCodes.html","page":"Running ITensor and Julia Codes","title":"Running ITensor and Julia Codes","text":"For more information on the above, see the following Julia Discourse post.","category":"page"},{"location":"getting_started/NextSteps.html#Next-Steps","page":"Next Steps","title":"Next Steps","text":"","category":"section"},{"location":"getting_started/NextSteps.html","page":"Next Steps","title":"Next Steps","text":"Browse the Code Examples.\nRead the ITensor Paper for a long-form introduction to the design and main features of the ITensor library\nRefer to the ITensorMPS.jl documentation for information on running finite MPS/MPO calculations such as DMRG.\nMore Julia language tutorials and resources\nFrom zero to Julia!\nThink Julia\nOfficial Julia Language Manual\nList of Resources at julialang.org","category":"page"},{"location":"RunningOnGPUs.html#Running-on-GPUs","page":"Running on GPUs","title":"Running on GPUs","text":"","category":"section"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"ITensor provides package extensions for running tensor operations on a variety of GPU backends. You can activate a backend by loading the appropriate Julia GPU package alongside ITensors.jl and moving your tensors and/or tensor networks to an available GPU using that package's provided conversion functions.","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"For example, you can load CUDA.jl to perform tensor operations on NVIDIA GPUs or Metal.jl to perform tensor operations on Apple GPUs:","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"using ITensors\n\ni, j, k = Index.((2, 2, 2))\nA = random_itensor(i, j)\nB = random_itensor(j, k)\n\n# Perform tensor operations on CPU\nA * B\n\n###########################################\nusing CUDA # This will trigger the loading of `NDTensorsCUDAExt` in the background\n\n# Move tensors to NVIDIA GPU\nAcu = cu(A)\nBcu = cu(B)\n\n# Perform tensor operations on NVIDIA GPU\nAcu * Bcu\n\n###########################################\nusing Metal # This will trigger the loading of `NDTensorsMetalExt` in the background\n\n# Move tensors to Apple GPU\nAmtl = mtl(A)\nBmtl = mtl(B)\n\n# Perform tensor operations on Apple GPU\nAmtl * Bmtl","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"Note that we highly recommend using these new package extensions as opposed to ITensorGPU.jl, which is ITensor's previous CUDA backend. The package extensions are better integrated into the main library so are more reliable and better supported right now. We plan to deprecate ITensorGPU.jl in the future.","category":"page"},{"location":"RunningOnGPUs.html#GPU-backends","page":"Running on GPUs","title":"GPU backends","text":"","category":"section"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"ITensor currently provides package extensions for the following GPU backends:","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"CUDA.jl (NVIDIA GPUs)\ncuTENSOR.jl (CUDA.jl extension providing accelerated binary tensor contractions)\nMetal.jl (Apple GPUs)\nAMDGPU.jl (AMD GPUs)","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"Our goal is to support all GPU backends which are supported by the JuliaGPU organization.","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"Notice that cuTENSOR.jl is an extension of CUDA.jl that provides new functionality for accelerated binary tensor contractions. If the cuTENSOR.jl library is loaded then ITensors with CuArray data are contracted using cuTENSOR and if the cuTENSOR.jl library is not loaded but CUDA.jl is loaded then binary tensor contractions are mapped to a matrix multiplication and performed using cuBLAS.","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"Some important caveats to keep in mind related to the ITensor GPU backends are:","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"only dense tensor operations are well supported right now. Block sparse operations (which arise when QN conservation is enabled) are under active development and either may not work or may be slower than their CPU counterparts,\ncertain GPU backends do not have native support for certain matrix decompositions like svd, eigen, and qr in which case we will perform those operations on CPU. If your calculation is dominated by those operations, there likely is no advantage to running it on GPU right now. CUDA generally has good support for native matrix decompositions, while Metal and AMD have more limited support right now, and\nsingle precision (Float32) calculations are generally fastest on GPU.","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"The table below summarizes each backend's current capabilities.","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":" CUDA cuTENSOR ROCm Metal oneAPI\nContractions (dense) ✓ (cuBLAS) ✓ ✓ ✓ N/A[oneapi]\nQR (dense) ✓ (cuSOLVER) ✓ (cuSOLVER) On CPU[linalg] On CPU[linalg] N/A[oneapi]\nSVD (dense) ✓ (cuSOLVER) ✓ (cuSOLVER) On CPU[linalg] On CPU[linalg] N/A[oneapi]\nEigendecomposition (dense) ✓ (cuSOLVER) ✓ (cuSOLVER) On CPU[linalg] On CPU[linalg] N/A[oneapi]\nDouble precision (Float64) ✓ ✓ ✓ N/A[metal] N/A[oneapi]\nBlock sparse ✓[blocksparse] ✓[blocksparse] ✓[blocksparse] ✓[blocksparse] N/A[oneapi]","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"[linalg]: Some GPU vendors have not implemented certain matrix factorizations, or the ones they have implemented are not efficient compared to running on CPU, so as a workaround we perform those operations on CPU by transferring the data back and forth from GPU to CPU. We will add support for running those operations on GPU as they become available. If your algorithm's cost is dominated by those operations you won't see any speedup by trying to run it on those kinds of GPUs.","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"[blocksparse]: Support is experimental. Operations may not be fully optimized and could have bugs.","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"[oneapi]: We plan to add Intel GPU support through Julia's oneAPI.jl interface but don't have any Intel GPUs to test on right now.","category":"page"},{"location":"RunningOnGPUs.html","page":"Running on GPUs","title":"Running on GPUs","text":"[metal]: Apple doesn't support double precision floating point operations on their GPUs, see Section 2.1 of the Metal Shading Language Specification. Until it does, we can't support double precision operations on Apple GPUs.","category":"page"},{"location":"Multithreading.html#Multithreading","page":"Multithreading","title":"Multithreading","text":"","category":"section"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"Most modern computers, including laptops, have multiple cores (processing units) which can be used to perform multiple tasks at the same time and therefore speed up computations. Multithreading is a form of shared memory parallelism that makes use of these multiple cores that you may have available.","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"There are three primary sources of parallelization available to ITensors.jl. These are:","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"BLAS/LAPACK multithreading (through whatever flavor you are using, i.e. OpenBLAS or MKL).\nThe Strided.jl package, which implements efficient multithreaded dense array permutations.\nBlock sparse multithreading (currently only for block sparse contractions) implemented in the NDTensors.jl package.","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"First, you can obtain the number of threads that are available to you with:","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"julia> Sys.CPU_THREADS\n6","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"If your computations are dominated by large dense tensors, you likely want to make use of BLAS multithreading in order to multithread dense matrix multiplications and other linear algebra methods like SVD and QR decompositions. This will be on by default. The BLAS/LAPACK multithreading can be controlled in the usual way with environment variables such as by starting Julia with:","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"$ MKL_NUM_THREADS=4 julia # Set the number of MKL threads to 4\n\n$ OPENBLAS_NUM_THREADS=4 julia # Set the number of OpenBLAS threads to 4\n\n$ OMP_NUM_THREADS=4 julia # Set the number of OpenMP threads to 4, which will be used by MKL or OpenBLAS if they are not specifically set","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"or at runtime from within Julia:","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"julia> using LinearAlgebra\n\njulia> BLAS.vendor()  # Check which BLAS you are using\n:mkl\n\njulia> BLAS.get_num_threads()\n6\n\njulia> BLAS.set_num_threads(4)\n\njulia> BLAS.get_num_threads()\n4","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"Note that in Julia v1.6, you will be able to use the command using LinearAlgebra; BLAS.get_num_threads().","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"We would highly recommend using MKL (see the installation instructions for how to do that), especially if you are using an Intel chip. How well BLAS multithreading will work depends on how much your calculations are dominated by large dense matrix operations (which is not always the case, especially if you are using QN conservation).","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"Currently, ITensors.jl makes use of the package Strided.jl for performant dense array permutations. It also provides multithreaded array permutations. If you start Julia with multiple threads, Strided multithreading is on by default:","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"$ julia -t 4\n\njulia> Threads.nthreads()\n4\n\njulia> using Strided\n\njulia> Strided.get_num_threads()\n4","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"We find that this threading competes with BLAS threading as well as ITensors.jl's own block sparse multithreading, so if you are using Julia with multiple threads you may want to disable Strided.jl's threading with:","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"julia> Strided.disable_threads()\n1\n\njulia> Strided.get_num_threads()\n1","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"in favor of either BLAS threading or ITensors.jl's block sparse threading.","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"Additionally, ITensors.jl, through the NDTensors.jl library, provides multithreaded block sparse operations. By default, this kind of threading is disabled. If your computations involve QN conserving tensors, you may want to consider enabling block sparse multithreading as described below.","category":"page"},{"location":"Multithreading.html#ITensors.enable_threaded_blocksparse","page":"Multithreading","title":"ITensors.enable_threaded_blocksparse","text":"ITensors.enable_threaded_blocksparse()\nITensors.disable_threaded_blocksparse()\n\nEnable or disable block sparse multithreading.\n\nReturns the current state of ITensors.using_threaded_blocksparse(), i.e. true if threaded block sparse was previously enabled, and false if threaded block sparse was previously disabled. This is helpful for turning block sparse threading on or off temporarily. For example:\n\nusing_threaded_blocksparse = ITensors.enable_threaded_blocksparse()\n# Run code that you want to be threaded\nif !using_threaded_blocksparse\n  ITensors.disable_threaded_blocksparse()\nend\n\nNote that you need to start Julia with multiple threads. For example, to start Julia with 4 threads, you can use any of the following:\n\n$ julia --threads=4\n\n$ julia -t 4\n\n$ JULIA_NUM_THREADS=4 julia\n\nIn addition, we have found that it is best to disable BLAS and Strided multithreading when using block sparse multithreading. You can do that with the commands using LinearAlgebra; BLAS.set_num_threads(1) and ITensors.Strided.disable_threads().\n\nSee also: ITensors.enable_threaded_blocksparse, ITensors.disable_threaded_blocksparse, ITensors.using_threaded_blocksparse.\n\n\n\n\n\nenable_threaded_blocksparse(enable::Bool)\n\nenable_threaded_blocksparse(true) enables threaded block sparse operations (equivalent to enable_threaded_blocksparse()).\n\nenable_threaded_blocksparse(false) disables threaded block sparse operations (equivalent to enable_threaded_blocksparse()).\n\n\n\n\n\n","category":"function"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"Here is a simple example of using block sparse multithreading to speed up a sparse tensor contraction:","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"using BenchmarkTools\nusing ITensors\nusing LinearAlgebra\nusing Strided\n\nfunction main(; d = 20, order = 4)\n  BLAS.set_num_threads(1)\n  Strided.set_num_threads(1)\n\n  println(\"#################################################\")\n  println(\"# order = \", order)\n  println(\"# d = \", d)\n  println(\"#################################################\")\n  println()\n\n  i(n) = Index(QN(0) => d, QN(1) => d; tags = \"i$n\")\n  is = ntuple(i, order ÷ 2)\n  A = random_itensor(is'..., dag(is)...)\n  B = random_itensor(is'..., dag(is)...)\n\n  ITensors.enable_threaded_blocksparse(false)\n\n  println(\"Serial contract:\")\n  @disable_warn_order begin\n    C_contract = @btime $A' * $B samples = 5\n  end\n  println()\n\n  println(\"Threaded contract:\")\n  @disable_warn_order begin\n    ITensors.enable_threaded_blocksparse(true)\n    C_threaded_contract = @btime $A' * $B samples = 5\n    ITensors.enable_threaded_blocksparse(false)\n  end\n  println()\n  @show C_contract ≈ C_threaded_contract\n  return nothing\nend\n\nmain(d = 20, order = 4)","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"which outputs the following on a laptop with 6 threads, starting Julia with 5 threads:","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"julia> main(d = 20, order = 4)\n#################################################\n# order = 4\n# d = 20\n#################################################\n\nThreads.nthreads() = 5\nSys.CPU_THREADS = 6\nBLAS.get_num_threads() = 1\nStrided.get_num_threads() = 1\n\nSerial contract:\n  21.558 ms (131 allocations: 7.34 MiB)\n\nThreaded contract:\n  5.934 ms (446 allocations: 7.37 MiB)\n\nC_contract ≈ C_threaded_contract = true","category":"page"},{"location":"Multithreading.html","page":"Multithreading","title":"Multithreading","text":"In addition, we plan to add more threading to other parts of the code beyond contraction (such as SVD) and improve composibility with other forms of threading like BLAS and Strided, so stay tuned!","category":"page"},{"location":"faq/Development.html#ITensor-Development-Frequently-Asked-Questions","page":"ITensor Development FAQs","title":"ITensor Development Frequently Asked Questions","text":"","category":"section"},{"location":"faq/Development.html#What-are-the-steps-to-contribute-code-to-ITensor?","page":"ITensor Development FAQs","title":"What are the steps to contribute code to ITensor?","text":"","category":"section"},{"location":"faq/Development.html","page":"ITensor Development FAQs","title":"ITensor Development FAQs","text":"Please contact us (support at itensor.org) if you are planning to submit a major contribution (more than a few lines of code, say). If so, we would like to discuss your plan and design before you spend significant time on it, to increase the chances we will merge your pull request.\nFork the ITensors.jl Github repo, create a new branch and make changes (commits) on that branch. ITensor imposes code formatting for contributions. Please run using JuliaFormatter; format(\".\") in the project directory to ensure formatting. As an alternative you may also use pre-commit. Install pre-commit with e.g. pip install pre-commit, then run pre-commit install in the project directory in order for pre-commit to run automatically before any commit.\nRun the ITensor unit tests by going into the test/ folder and running julia runtests.jl. To run individual test scripts, start a Julia REPL (interactive terminal) session and include each script, such as include(\"itensor.jl\").\nPush your new branch and changes to your forked repo. Github will give you the option to make a pull request (PR) out of your branch that will be submitted to us, and which you can view under the list of ITensors.jl pull requests. If your PR's tests pass and we approve your changes, we will merge it or ask you to merge it. If you merge your PR, please use the Squash and Merge option. We may also ask you to make more changes to bring your PR in line with our design goals or technical requirements.","category":"page"},{"location":"ITensorType.html#ITensor","page":"ITensor","title":"ITensor","text":"","category":"section"},{"location":"ITensorType.html#Description","page":"ITensor","title":"Description","text":"","category":"section"},{"location":"ITensorType.html#ITensors.ITensor","page":"ITensor","title":"ITensors.ITensor","text":"ITensor\n\nAn ITensor is a tensor whose interface is independent of its memory layout. Therefore it is not necessary to know the ordering of an ITensor's indices, only which indices an ITensor has. Operations like contraction and addition of ITensors automatically handle any memory permutations.\n\nExamples\n\njulia> i = Index(2, \"i\")\n(dim=2|id=287|\"i\")\n\n#\n# Make an ITensor with random elements:\n#\njulia> A = random_itensor(i', i)\nITensor ord=2 (dim=2|id=287|\"i\")' (dim=2|id=287|\"i\")\nNDTensors.Dense{Float64,Array{Float64,1}}\n\njulia> @show A;\nA = ITensor ord=2\nDim 1: (dim=2|id=287|\"i\")'\nDim 2: (dim=2|id=287|\"i\")\nNDTensors.Dense{Float64,Array{Float64,1}}\n 2×2\n 0.28358594718392427   1.4342219756446355\n 1.6620103556283987   -0.40952231269251566\n\njulia> @show inds(A);\ninds(A) = ((dim=2|id=287|\"i\")', (dim=2|id=287|\"i\"))\n\n#\n# Set the i==1, i'==2 element to 1.0:\n#\njulia> A[i => 1, i' => 2] = 1;\n\njulia> @show A;\nA = ITensor ord=2\nDim 1: (dim=2|id=287|\"i\")'\nDim 2: (dim=2|id=287|\"i\")\nNDTensors.Dense{Float64,Array{Float64,1}}\n 2×2\n 0.28358594718392427   1.4342219756446355\n 1.0                  -0.40952231269251566\n\njulia> @show storage(A);\nstorage(A) = [0.28358594718392427, 1.0, 1.4342219756446355, -0.40952231269251566]\n\njulia> B = random_itensor(i, i');\n\njulia> @show B;\nB = ITensor ord=2\nDim 1: (dim=2|id=287|\"i\")\nDim 2: (dim=2|id=287|\"i\")'\nNDTensors.Dense{Float64,Array{Float64,1}}\n 2×2\n -0.6510816500352691   0.2579101497658179\n  0.256266641521826   -0.9464735926768166\n\n#\n# Can add or subtract ITensors as long as they\n# have the same indices, in any order:\n#\njulia> @show A + B;\nA + B = ITensor ord=2\nDim 1: (dim=2|id=287|\"i\")'\nDim 2: (dim=2|id=287|\"i\")\nNDTensors.Dense{Float64,Array{Float64,1}}\n 2×2\n -0.3674957028513448   1.6904886171664615\n  1.2579101497658178  -1.3559959053693322\n\n\n\n\n\n","category":"type"},{"location":"ITensorType.html#Dense-Constructors","page":"ITensor","title":"Dense Constructors","text":"","category":"section"},{"location":"ITensorType.html#ITensors.ITensor-Tuple{Type{<:Number}, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.ITensor","text":"ITensor([::Type{ElT} = Float64, ]inds)\nITensor([::Type{ElT} = Float64, ]inds::Index...)\n\nConstruct an ITensor filled with zeros having indices inds and element type ElT. If the element type is not specified, it defaults to Float64.\n\nThe storage will have NDTensors.Dense type.\n\nExamples\n\ni = Index(2,\"index_i\")\nj = Index(4,\"index_j\")\nk = Index(3,\"index_k\")\n\nA = ITensor(i,j)\nB = ITensor(ComplexF64,k,j)\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.ITensor-Tuple{Type{<:Number}, UndefInitializer, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.ITensor","text":"ITensor([::Type{ElT} = Float64, ]::UndefInitializer, inds)\nITensor([::Type{ElT} = Float64, ]::UndefInitializer, inds::Index...)\n\nConstruct an ITensor filled with undefined elements having indices inds and element type ElT. If the element type is not specified, it defaults to Float64. One purpose for using this constructor is that initializing the elements in an   undefined way is faster than initializing them to a set value such as zero.\n\nThe storage will have NDTensors.Dense type.\n\nExamples\n\ni = Index(2,\"index_i\")\nj = Index(4,\"index_j\")\nk = Index(3,\"index_k\")\n\nA = ITensor(undef,i,j)\nB = ITensor(ComplexF64,undef,k,j)\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.ITensor-Tuple{Type{<:Number}, Number, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.ITensor","text":"ITensor([ElT::Type, ]x::Number, inds)\nITensor([ElT::Type, ]x::Number, inds::Index...)\n\nConstruct an ITensor with all elements set to x and indices inds.\n\nIf x isa Int or x isa Complex{Int} then the elements will be set to float(x)   unless specified otherwise by the first input.\n\nThe storage will have NDTensors.Dense type.\n\nExamples\n\n```julia   i = Index(2,\"indexi\"); j = Index(4,\"indexj\"); k = Index(3,\"index_k\");\n\nA = ITensor(1.0, i, j)   A = ITensor(1, i, j) # same as above   B = ITensor(2.0+3.0im, j, k)   ```\n\n!!! warning       In future versions this may not automatically convert integer inputs with float, and in that case the particular element type should not be relied on.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.ITensor-Tuple{NDTensors.AliasStyle, Type{<:Number}, Array{<:Number}, Union{Tuple{Vararg{Index{Int64}}}, Vector{Index{Int64}}}}","page":"ITensor","title":"ITensors.ITensor","text":"ITensor([ElT::Type, ]A::AbstractArray, inds)\nITensor([ElT::Type, ]A::AbstractArray, inds::Index...)\n\nitensor([ElT::Type, ]A::AbstractArray, inds)\nitensor([ElT::Type, ]A::AbstractArray, inds::Index...)\n\nConstruct an ITensor from an AbstractArray A and indices inds. The ITensor will be a view of the AbstractArray data if possible (if no conversion to a different element type is necessary).\n\nIf specified, the ITensor will have element type ElT.\n\nIf the element type of A is Int or Complex{Int} and the desired element type isn't specified, it will be converted to Float64 or Complex{Float64} automatically. To keep the element type as an integer, specify it explicitly, for example with:\n\ni = Index(2, \"i\")\nA = [0 1; 1 0]\nT = ITensor(eltype(A), A, i', dag(i))\n\nExamples\n\ni = Index(2,\"index_i\")\nj = Index(2,\"index_j\")\n\nM = [1. 2;\n     3 4]\nT = ITensor(M, i, j)\nT[i => 1, j => 1] = 3.3\nM[1, 1] == 3.3\nT[i => 1, j => 1] == 3.3\n\nwarning: Warning\nIn future versions this may not automatically convert Int/Complex{Int} inputs to floating point versions with float (once tensor operations using Int/Complex{Int} are natively as fast as floating point operations), and in that case the particular element type should not be relied on. To avoid extra conversions (and therefore allocations) it is best practice to directly construct with itensor([0. 1; 1 0], i', dag(i)) if you want a floating point element type. The conversion is done as a performance optimization since often tensors are passed to BLAS/LAPACK and need to be converted to floating point types compatible with those libraries, but future projects in Julia may allow for efficient operations with more general element types (for example see https://github.com/JuliaLinearAlgebra/Octavian.jl).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.random_itensor-Tuple{Type{<:Number}, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.random_itensor","text":"random_itensor([rng=Random.default_rng()], [ElT=Float64], inds)\nrandom_itensor([rng=Random.default_rng()], [ElT=Float64], inds::Index...)\n\nConstruct an ITensor with type ElT and indices inds, whose elements are normally distributed random numbers. If the element type is not specified, it defaults to Float64.\n\nExamples\n\ni = Index(2,\"index_i\")\nj = Index(4,\"index_j\")\nk = Index(3,\"index_k\")\n\nA = random_itensor(i,j)\nB = random_itensor(ComplexF64,undef,k,j)\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.onehot","page":"ITensor","title":"ITensors.onehot","text":"onehot(ivs...)\nsetelt(ivs...)\nonehot(::Type, ivs...)\nsetelt(::Type, ivs...)\n\nCreate an ITensor with all zeros except the specified value, which is set to 1.\n\nExamples\n\ni = Index(2,\"i\")\nA = onehot(i=>2)\n# A[i=>2] == 1, all other elements zero\n\n# Specify the element type\nA = onehot(Float32, i=>2)\n\nj = Index(3,\"j\")\nB = onehot(i=>1,j=>3)\n# B[i=>1,j=>3] == 1, all other element zero\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#Dense-View-Constructors","page":"ITensor","title":"Dense View Constructors","text":"","category":"section"},{"location":"ITensorType.html#ITensors.itensor-Tuple{Array{<:Number}, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.itensor","text":"itensor(args...; kwargs...)\n\nLike the ITensor constructor, but with attempt to make a view of the input data when possible.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#QN-BlockSparse-Constructors","page":"ITensor","title":"QN BlockSparse Constructors","text":"","category":"section"},{"location":"ITensorType.html#ITensors.ITensor-Tuple{Type{<:Number}, QN, Union{Tuple{Vararg{Index{Vector{Pair{QN, Int64}}}}}, Vector{Index{Vector{Pair{QN, Int64}}}}}}","page":"ITensor","title":"ITensors.ITensor","text":"ITensor([::Type{ElT} = Float64, ][flux::QN = QN(), ]inds)\nITensor([::Type{ElT} = Float64, ][flux::QN = QN(), ]inds::Index...)\n\nConstruct an ITensor with BlockSparse storage filled with zero(ElT) where the nonzero blocks are determined by flux.\n\nIf ElT is not specified it defaults to Float64.\n\nIf flux is not specified, the ITensor will be empty (it will contain no blocks, and have an undefined flux). The flux will be set by the first element that is set.\n\nExamples\n\njulia> i\n(dim=3|id=212|\"i\") <Out>\n 1: QN(0) => 1\n 2: QN(1) => 2\n\njulia> @show ITensor(QN(0), i', dag(i));\nITensor(QN(0), i', dag(i)) = ITensor ord=2\nDim 1: (dim=3|id=212|\"i\")' <Out>\n 1: QN(0) => 1\n 2: QN(1) => 2\nDim 2: (dim=3|id=212|\"i\") <In>\n 1: QN(0) => 1\n 2: QN(1) => 2\nNDTensors.BlockSparse{Float64, Vector{Float64}, 2}\n 3×3\nBlock(1, 1)\n [1:1, 1:1]\n 0.0\n\nBlock(2, 2)\n [2:3, 2:3]\n 0.0  0.0\n 0.0  0.0\n\njulia> @show ITensor(QN(1), i', dag(i));\nITensor(QN(1), i', dag(i)) = ITensor ord=2\nDim 1: (dim=3|id=212|\"i\")' <Out>\n 1: QN(0) => 1\n 2: QN(1) => 2\nDim 2: (dim=3|id=212|\"i\") <In>\n 1: QN(0) => 1\n 2: QN(1) => 2\nNDTensors.BlockSparse{Float64, Vector{Float64}, 2}\n 3×3\nBlock(2, 1)\n [2:3, 1:1]\n 0.0\n 0.0\n\njulia> @show ITensor(ComplexF64, QN(1), i', dag(i));\nITensor(ComplexF64, QN(1), i', dag(i)) = ITensor ord=2\nDim 1: (dim=3|id=212|\"i\")' <Out>\n 1: QN(0) => 1\n 2: QN(1) => 2\nDim 2: (dim=3|id=212|\"i\") <In>\n 1: QN(0) => 1\n 2: QN(1) => 2\nNDTensors.BlockSparse{ComplexF64, Vector{ComplexF64}, 2}\n 3×3\nBlock(2, 1)\n [2:3, 1:1]\n 0.0 + 0.0im\n 0.0 + 0.0im\n\njulia> @show ITensor(undef, QN(1), i', dag(i));\nITensor(undef, QN(1), i', dag(i)) = ITensor ord=2\nDim 1: (dim=3|id=212|\"i\")' <Out>\n 1: QN(0) => 1\n 2: QN(1) => 2\nDim 2: (dim=3|id=212|\"i\") <In>\n 1: QN(0) => 1\n 2: QN(1) => 2\nNDTensors.BlockSparse{Float64, Vector{Float64}, 2}\n 3×3\nBlock(2, 1)\n [2:3, 1:1]\n 0.0\n 1.63e-322\n\nConstruction with undefined flux:\n\njulia> A = ITensor(i', dag(i));\n\njulia> @show A;\nA = ITensor ord=2\nDim 1: (dim=3|id=212|\"i\")' <Out>\n 1: QN(0) => 1\n 2: QN(1) => 2\nDim 2: (dim=3|id=212|\"i\") <In>\n 1: QN(0) => 1\n 2: QN(1) => 2\nNDTensors.EmptyStorage{NDTensors.EmptyNumber, NDTensors.BlockSparse{NDTensors.EmptyNumber, Vector{NDTensors.EmptyNumber}, 2}}\n 3×3\n\n\n\njulia> isnothing(flux(A))\ntrue\n\njulia> A[i' => 1, i => 2] = 2\n2\n\njulia> @show A;\nA = ITensor ord=2\nDim 1: (dim=3|id=212|\"i\")' <Out>\n 1: QN(0) => 1\n 2: QN(1) => 2\nDim 2: (dim=3|id=212|\"i\") <In>\n 1: QN(0) => 1\n 2: QN(1) => 2\nNDTensors.BlockSparse{Int64, Vector{Int64}, 2}\n 3×3\nBlock(1, 2)\n [1:1, 2:3]\n 2  0\n\njulia> flux(A)\nQN(-1)\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.ITensor-Tuple{NDTensors.AliasStyle, Type{<:Number}, Array{<:Number}, Union{Tuple{Vararg{Index{Vector{Pair{QN, Int64}}}}}, Vector{Index{Vector{Pair{QN, Int64}}}}}}","page":"ITensor","title":"ITensors.ITensor","text":"ITensor([ElT::Type, ]A::AbstractArray, inds)\nITensor([ElT::Type, ]A::AbstractArray, inds::Index...)\n\nitensor([ElT::Type, ]A::AbstractArray, inds)\nitensor([ElT::Type, ]A::AbstractArray, inds::Index...)\n\nConstruct an ITensor from an AbstractArray A and indices inds. The ITensor will be a view of the AbstractArray data if possible (if no conversion to a different element type is necessary).\n\nIf specified, the ITensor will have element type ElT.\n\nIf the element type of A is Int or Complex{Int} and the desired element type isn't specified, it will be converted to Float64 or Complex{Float64} automatically. To keep the element type as an integer, specify it explicitly, for example with:\n\ni = Index(2, \"i\")\nA = [0 1; 1 0]\nT = ITensor(eltype(A), A, i', dag(i))\n\nExamples\n\ni = Index(2,\"index_i\")\nj = Index(2,\"index_j\")\n\nM = [1. 2;\n     3 4]\nT = ITensor(M, i, j)\nT[i => 1, j => 1] = 3.3\nM[1, 1] == 3.3\nT[i => 1, j => 1] == 3.3\n\nwarning: Warning\nIn future versions this may not automatically convert Int/Complex{Int} inputs to floating point versions with float (once tensor operations using Int/Complex{Int} are natively as fast as floating point operations), and in that case the particular element type should not be relied on. To avoid extra conversions (and therefore allocations) it is best practice to directly construct with itensor([0. 1; 1 0], i', dag(i)) if you want a floating point element type. The conversion is done as a performance optimization since often tensors are passed to BLAS/LAPACK and need to be converted to floating point types compatible with those libraries, but future projects in Julia may allow for efficient operations with more general element types (for example see https://github.com/JuliaLinearAlgebra/Octavian.jl).\n\n\n\n\n\nITensor([ElT::Type, ]::AbstractArray, inds; tol=0.0, checkflux=true)\n\nCreate a block sparse ITensor from the input Array, and collection of QN indices. Zeros are dropped and nonzero blocks are determined from the zero values of the array.\n\nOptionally, you can set a tolerance such that elements less than or equal to the tolerance are dropped.\n\nBy default, this will check that the flux of the nonzero blocks are consistent with each other. You can disable this check by setting checkflux=false.\n\nExamples\n\njulia> i = Index([QN(0)=>1, QN(1)=>2], \"i\");\n\njulia> A = [1e-9 0.0 0.0;\n            0.0 2.0 3.0;\n            0.0 1e-10 4.0];\n\njulia> @show ITensor(A, i', dag(i); tol = 1e-8);\nITensor(A, i', dag(i); tol = 1.0e-8) = ITensor ord=2\nDim 1: (dim=3|id=468|\"i\")' <Out>\n 1: QN(0) => 1\n 2: QN(1) => 2\nDim 2: (dim=3|id=468|\"i\") <In>\n 1: QN(0) => 1\n 2: QN(1) => 2\nNDTensors.BlockSparse{Float64,Array{Float64,1},2}\n 3×3\nBlock: (2, 2)\n [2:3, 2:3]\n 2.0  3.0\n 0.0  4.0\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.ITensor-Tuple{Type{<:Number}, UndefInitializer, QN, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.ITensor","text":"ITensor([::Type{ElT} = Float64,] ::UndefInitializer, flux::QN, inds)\nITensor([::Type{ElT} = Float64,] ::UndefInitializer, flux::QN, inds::Index...)\n\nConstruct an ITensor with indices inds and BlockSparse storage with undefined elements of type ElT, where the nonzero (allocated) blocks are determined by the provided QN flux. One purpose for using this constructor is that initializing the elements in an undefined way is faster than initializing them to a set value such as zero.\n\nThe storage will have NDTensors.BlockSparse type.\n\nExamples\n\ni = Index([QN(0)=>1, QN(1)=>2], \"i\")\nA = ITensor(undef,QN(0),i',dag(i))\nB = ITensor(Float64,undef,QN(0),i',dag(i))\nC = ITensor(ComplexF64,undef,QN(0),i',dag(i))\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Diagonal-constructors","page":"ITensor","title":"Diagonal constructors","text":"","category":"section"},{"location":"ITensorType.html#ITensors.diag_itensor-Tuple{Type{<:Number}, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.diag_itensor","text":"diag_itensor([::Type{ElT} = Float64, ]inds)\ndiag_itensor([::Type{ElT} = Float64, ]inds::Index...)\n\nMake a sparse ITensor of element type ElT with only elements along the diagonal stored. Defaults to having zero(T) along the diagonal.\n\nThe storage will have NDTensors.Diag type.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.diag_itensor-Tuple{NDTensors.AliasStyle, Type{<:Number}, Vector{<:Number}, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.diag_itensor","text":"diag_itensor([ElT::Type, ]v::AbstractVector, inds...)\ndiagitensor([ElT::Type, ]v::AbstractVector, inds...)\n\nMake a sparse ITensor with non-zero elements only along the diagonal. In general, the diagonal elements will be those stored in v and the ITensor will have element type eltype(v), unless specified explicitly by ElT. The storage will have NDTensors.Diag type.\n\nIn the case when eltype(v) isa Union{Int, Complex{Int}}, by default it will be converted to float(v). Note that this behavior is subject to change in the future.\n\nThe version diag_itensor will never output an ITensor whose storage data is an alias of the input vector data.\n\nThe version diagitensor might output an ITensor whose storage data is an alias of the input vector data in order to minimize operations.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.diag_itensor-Tuple{NDTensors.AliasStyle, Type{<:Number}, Number, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.diag_itensor","text":"diag_itensor([ElT::Type, ]x::Number, inds...)\ndiagitensor([ElT::Type, ]x::Number, inds...)\n\nMake a sparse ITensor with non-zero elements only along the diagonal. In general, the diagonal elements will be set to the value x and the ITensor will have element type eltype(x), unless specified explicitly by ElT. The storage will have NDTensors.Diag type.\n\nIn the case when x isa Union{Int, Complex{Int}}, by default it will be converted to float(x). Note that this behavior is subject to change in the future.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.delta-Tuple{Type{<:Number}, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.delta","text":"delta([::Type{ElT} = Float64, ]inds)\ndelta([::Type{ElT} = Float64, ]inds::Index...)\n\nMake a uniform diagonal ITensor with all diagonal elements one(ElT). Only a single diagonal element is stored.\n\nThis function has an alias δ.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#QN-Diagonal-constructors","page":"ITensor","title":"QN Diagonal constructors","text":"","category":"section"},{"location":"ITensorType.html#ITensors.diag_itensor-Tuple{Type{<:Number}, QN, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.diag_itensor","text":"diag_itensor([::Type{ElT} = Float64, ][flux::QN = QN(), ]is)\ndiag_itensor([::Type{ElT} = Float64, ][flux::QN = QN(), ]is::Index...)\n\nMake an ITensor with storage type NDTensors.DiagBlockSparse with elements zero(ElT). The ITensor only has diagonal blocks consistent with the specified flux.\n\nIf the element type is not specified, it defaults to Float64. If theflux is not specified, it defaults to QN().\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.delta-Tuple{Type{<:Number}, QN, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}","page":"ITensor","title":"ITensors.delta","text":"delta([::Type{ElT} = Float64, ][flux::QN = QN(), ]is)\ndelta([::Type{ElT} = Float64, ][flux::QN = QN(), ]is::Index...)\n\nMake an ITensor with storage type NDTensors.DiagBlockSparse with uniform elements one(ElT). The ITensor only has diagonal blocks consistent with the specified flux.\n\nIf the element type is not specified, it defaults to Float64. If theflux is not specified, it defaults to QN().\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Convert-to-Array","page":"ITensor","title":"Convert to Array","text":"","category":"section"},{"location":"ITensorType.html#Core.Array-Union{Tuple{N}, Tuple{ElT}, Tuple{ITensor, Union{Tuple{Vararg{IndexT}}, Vector{IndexT}} where IndexT<:Index}} where {ElT, N}","page":"ITensor","title":"Core.Array","text":"Array{ElT, N}(T::ITensor, i:Index...)\nArray{ElT}(T::ITensor, i:Index...)\nArray(T::ITensor, i:Index...)\n\nMatrix{ElT}(T::ITensor, row_i:Index, col_i::Index)\nMatrix(T::ITensor, row_i:Index, col_i::Index)\n\nVector{ElT}(T::ITensor)\nVector(T::ITensor)\n\nGiven an ITensor T with indices i..., returns an Array with a copy of the ITensor's elements. The order in which the indices are provided indicates the order of the data in the resulting Array.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#NDTensors.array-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"NDTensors.array","text":"array(T::ITensor, inds...)\n\nConvert an ITensor T to an Array.\n\nThe ordering of the elements in the Array are specified by the input indices inds. This tries to avoid copying of possible (i.e. may return a view of the original data), for example if the ITensor's storage is Dense and the indices are already in the specified ordering so that no permutation is required.\n\nwarning: Warning\nNote that in the future we may return specialized AbstractArray types for certain storage types, for example a LinearAlgebra.Diagonal type for an ITensor with Diag storage. The specific storage type shouldn't be relied upon.\n\nSee also matrix, vector.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#NDTensors.matrix-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"NDTensors.matrix","text":"matrix(T::ITensor, inds...)\n\nConvert an ITensor T to a Matrix.\n\nThe ordering of the elements in the Matrix are specified by the input indices inds. This tries to avoid copying of possible (i.e. may return a view of the original data), for example if the ITensor's storage is Dense and the indices are already in the specified ordering so that no permutation is required.\n\nwarning: Warning\nNote that in the future we may return specialized AbstractArray types for certain storage types, for example a LinearAlgebra.Diagonal type for an ITensor with Diag storage. The specific storage type shouldn't be relied upon.\n\nSee also array, vector.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#NDTensors.vector-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"NDTensors.vector","text":"vector(T::ITensor, inds...)\n\nConvert an ITensor T to an Vector.\n\nThe ordering of the elements in the Array are specified by the input indices inds. This tries to avoid copying of possible (i.e. may return a view of the original data), for example if the ITensor's storage is Dense and the indices are already in the specified ordering so that no permutation is required.\n\nwarning: Warning\nNote that in the future we may return specialized AbstractArray types for certain storage types, for example a LinearAlgebra.Diagonal type for an ITensor with Diag storage. The specific storage type shouldn't be relied upon.\n\nSee also array, matrix.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#NDTensors.array-Tuple{ITensor}","page":"ITensor","title":"NDTensors.array","text":"array(T::ITensor)\n\nGiven an ITensor T, returns an Array with a copy of the ITensor's elements, or a view in the case the the ITensor's storage is Dense.\n\nThe ordering of the elements in the Array, in terms of which Index is treated as the row versus column, depends on the internal layout of the ITensor.\n\nwarning: Warning\nThis method is intended for developer use only and not recommended for use in ITensor applications unless you know what you are doing (for example you are certain of the memory ordering of the ITensor because you permuted the indices into a certain order).\n\nSee also matrix, vector.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#NDTensors.matrix-Tuple{ITensor}","page":"ITensor","title":"NDTensors.matrix","text":"matrix(T::ITensor)\n\nGiven an ITensor T with two indices, returns a Matrix with a copy of the ITensor's elements, or a view in the case the ITensor's storage is Dense.\n\nThe ordering of the elements in the Matrix, in terms of which Index is treated as the row versus column, depends on the internal layout of the ITensor.\n\nwarning: Warning\nThis method is intended for developer use only and not recommended for use in ITensor applications unless you know what you are doing (for example you are certain of the memory ordering of the ITensor because you permuted the indices into a certain order).\n\nSee also array, vector.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#NDTensors.vector-Tuple{ITensor}","page":"ITensor","title":"NDTensors.vector","text":"vector(T::ITensor)\n\nGiven an ITensor T with one index, returns a Vector with a copy of the ITensor's elements, or a view in the case the ITensor's storage is Dense.\n\nSee also array, matrix.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Getting-and-setting-elements","page":"ITensor","title":"Getting and setting elements","text":"","category":"section"},{"location":"ITensorType.html#Base.getindex-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"Base.getindex","text":"getindex(T::ITensor, ivs...)\n\nGet the specified element of the ITensor, using a list of IndexVals or Pair{<:Index, Int}.\n\nExample\n\ni = Index(2; tags = \"i\")\nA = ITensor(2.0, i, i')\nA[i => 1, i' => 2] # 2.0, same as: A[i' => 2, i => 1]\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Base.setindex!-Tuple{ITensor, Number, Vararg{Int64}}","page":"ITensor","title":"Base.setindex!","text":"setindex!(T::ITensor, x::Number, ivs...)\n\nsetindex!(T::ITensor, x::Number, I::Integer...)\n\nsetindex!(T::ITensor, x::Number, I::CartesianIndex)\n\nSet the specified element of the ITensor, using a list of Pair{<:Index, Integer} (or IndexVal).\n\nIf just integers are used, set the specified element of the ITensor using internal Index ordering of the ITensor (only for advanced usage, only use if you know the axact ordering of the indices).\n\nExample\n\ni = Index(2; tags = \"i\")\nA = ITensor(i, i')\nA[i => 1, i' => 2] = 1.0 # same as: A[i' => 2, i => 1] = 1.0\nA[1, 2] = 1.0 # same as: A[i => 1, i' => 2] = 1.0\n\n# Some simple slicing is also supported\nA[i => 2, i' => :] = [2.0 3.0]\nA[2, :] = [2.0 3.0]\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Properties","page":"ITensor","title":"Properties","text":"","category":"section"},{"location":"ITensorType.html#NDTensors.inds-Tuple{ITensor}","page":"ITensor","title":"NDTensors.inds","text":"inds(T::ITensor)\n\nReturn the indices of the ITensor as a Tuple.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#NDTensors.ind-Tuple{ITensor, Int64}","page":"ITensor","title":"NDTensors.ind","text":"ind(T::ITensor, i::Int)\n\nGet the Index of the ITensor along dimension i.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.dir-Tuple{ITensor, Index}","page":"ITensor","title":"ITensors.dir","text":"dir(A::ITensor, i::Index)\n\nReturn the direction of the Index i in the ITensor A.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Priming_and_tagging_ITensor","page":"ITensor","title":"Priming and tagging","text":"","category":"section"},{"location":"ITensorType.html#ITensors.prime-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.prime","text":"prime[!](A::ITensor, plinc::Int = 1; <keyword arguments>) -> ITensor\n\nprime(inds, plinc::Int = 1; <keyword arguments>) -> IndexSet\n\nIncrease the prime level of the indices of an ITensor or collection of indices.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.setprime-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.setprime","text":"setprime[!](A::ITensor, plev::Int; <keyword arguments>) -> ITensor\n\nsetprime(inds, plev::Int; <keyword arguments>) -> IndexSet\n\nSet the prime level of the indices of an ITensor or collection of indices.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.noprime-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.noprime","text":"noprime[!](A::ITensor; <keyword arguments>) -> ITensor\n\nnoprime(inds; <keyword arguments>) -> IndexSet\n\nSet the prime level of the indices of an ITensor or collection of indices to zero.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.mapprime-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.mapprime","text":"replaceprime[!](A::ITensor, plold::Int, plnew::Int; <keyword arguments>) -> ITensor\nreplaceprime[!](A::ITensor, plold => plnew; <keyword arguments>) -> ITensor\nmapprime[!](A::ITensor, <arguments>; <keyword arguments>) -> ITensor\n\nreplaceprime(inds, plold::Int, plnew::Int; <keyword arguments>)\nreplaceprime(inds::IndexSet, plold => plnew; <keyword arguments>)\nmapprime(inds, <arguments>; <keyword arguments>)\n\nSet the prime level of the indices of an ITensor or collection of indices with prime level plold to plnew.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.swapprime-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.swapprime","text":"swapprime[!](A::ITensor, pl1::Int, pl2::Int; <keyword arguments>) -> ITensor\nswapprime[!](A::ITensor, pl1 => pl2; <keyword arguments>) -> ITensor\n\nswapprime(inds, pl1::Int, pl2::Int; <keyword arguments>)\nswapprime(inds, pl1 => pl2; <keyword arguments>)\n\nSet the prime level of the indices of an ITensor or collection of indices with prime level pl1 to pl2, and those with prime level pl2 to pl1.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.TagSets.addtags-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.TagSets.addtags","text":"addtags[!](A::ITensor, ts::String; <keyword arguments>) -> ITensor\n\naddtags(inds, ts::String; <keyword arguments>)\n\nAdd the tags ts to the indices of an ITensor or collection of indices.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.TagSets.removetags-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.TagSets.removetags","text":"removetags[!](A::ITensor, ts::String; <keyword arguments>) -> ITensor\n\nremovetags(inds, ts::String; <keyword arguments>)\n\nRemove the tags ts from the indices of an ITensor or collection of indices.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.TagSets.replacetags-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.TagSets.replacetags","text":"replacetags[!](A::ITensor, tsold::String, tsnew::String; <keyword arguments>) -> ITensor\n\nreplacetags(is::IndexSet, tsold::String, tsnew::String; <keyword arguments>) -> IndexSet\n\nReplace the tags tsold with tsnew for the indices of an ITensor.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.settags-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.settags","text":"settags[!](A::ITensor, ts::String; <keyword arguments>) -> ITensor\n\nsettags(is::IndexSet, ts::String; <keyword arguments>) -> IndexSet\n\nSet the tags of the indices of an ITensor or IndexSet to ts.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.swaptags-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.swaptags","text":"swaptags[!](A::ITensor, ts1::String, ts2::String; <keyword arguments>) -> ITensor\n\nswaptags(is::IndexSet, ts1::String, ts2::String; <keyword arguments>) -> IndexSet\n\nSwap the tags ts1 with ts2 for the indices of an ITensor.\n\nOptionally, only modify the indices with the specified keyword arguments.\n\nArguments\n\ntags = nothing: if specified, only modify Index i if hastags(i, tags) == true.\nplev = nothing: if specified, only modify Index i if hasplev(i, plev) == true.\n\nThe ITensor functions come in two versions, f and f!. The latter modifies the ITensor in-place. In both versions, the ITensor storage is not modified or copied (so it returns an ITensor with a view of the original storage).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Index-collections-set-operations","page":"ITensor","title":"Index collections set operations","text":"","category":"section"},{"location":"ITensorType.html#ITensors.commoninds","page":"ITensor","title":"ITensors.commoninds","text":"commoninds(A, B; kwargs...)\n\nReturn a Vector with indices that are common between the indices of A and B (the set intersection, similar to Base.intersect).\n\nOptional keyword arguments:\n\ntags::String - a tag name or comma separated list of tag names that the returned indices must all have\nplev::Int - common prime level that the returned indices must all have\ninds - Index or collection of indices. Returned indices must come from this set of indices.\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#ITensors.commonind","page":"ITensor","title":"ITensors.commonind","text":"commonind(A, B; kwargs...)\n\nReturn the first Index common between the indices of A and B.\n\nSee also commoninds.\n\nOptional keyword arguments:\n\ntags::String - a tag name or comma separated list of tag names that the returned indices must all have\nplev::Int - common prime level that the returned indices must all have\ninds - Index or collection of indices. Returned indices must come from this set of indices.\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#ITensors.uniqueinds","page":"ITensor","title":"ITensors.uniqueinds","text":"uniqueinds(A, B; kwargs...)\n\nReturn Vector with indices that are unique to the set of indices of A and not in B (the set difference, similar to Base.setdiff).\n\nOptional keyword arguments:\n\ntags::String - a tag name or comma separated list of tag names that the returned indices must all have\nplev::Int - common prime level that the returned indices must all have\ninds - Index or collection of indices. Returned indices must come from this set of indices.\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#ITensors.uniqueind","page":"ITensor","title":"ITensors.uniqueind","text":"uniqueind(A, B; kwargs...)\n\nReturn the first Index unique to the set of indices of A and not in B.\n\nSee also uniqueinds.\n\nOptional keyword arguments:\n\ntags::String - a tag name or comma separated list of tag names that the returned indices must all have\nplev::Int - common prime level that the returned indices must all have\ninds - Index or collection of indices. Returned indices must come from this set of indices.\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#ITensors.noncommoninds","page":"ITensor","title":"ITensors.noncommoninds","text":"noncommoninds(A, B; kwargs...)\n\nReturn a Vector with indices that are not common between the indices of A and B (the symmetric set difference, similar to Base.symdiff).\n\nOptional keyword arguments:\n\ntags::String - a tag name or comma separated list of tag names that the returned indices must all have\nplev::Int - common prime level that the returned indices must all have\ninds - Index or collection of indices. Returned indices must come from this set of indices.\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#ITensors.noncommonind","page":"ITensor","title":"ITensors.noncommonind","text":"noncommonind(A, B; kwargs...)\n\nReturn the first Index not common between the indices of A and B.\n\nSee also noncommoninds.\n\nOptional keyword arguments:\n\ntags::String - a tag name or comma separated list of tag names that the returned indices must all have\nplev::Int - common prime level that the returned indices must all have\ninds - Index or collection of indices. Returned indices must come from this set of indices.\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#ITensors.unioninds","page":"ITensor","title":"ITensors.unioninds","text":"unioninds(A, B; kwargs...)\n\nReturn a Vector with indices that are the union of the indices of A and B (the set union, similar to Base.union).\n\nOptional keyword arguments:\n\ntags::String - a tag name or comma separated list of tag names that the returned indices must all have\nplev::Int - common prime level that the returned indices must all have\ninds - Index or collection of indices. Returned indices must come from this set of indices.\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#ITensors.unionind","page":"ITensor","title":"ITensors.unionind","text":"unionind(A, B; kwargs...)\n\nReturn the first Index in the union of the indices of A and B.\n\nSee also unioninds.\n\nOptional keyword arguments:\n\ntags::String - a tag name or comma separated list of tag names that the returned indices must all have\nplev::Int - common prime level that the returned indices must all have\ninds - Index or collection of indices. Returned indices must come from this set of indices.\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#ITensors.hascommoninds","page":"ITensor","title":"ITensors.hascommoninds","text":"hascommoninds(A, B; kwargs...)\n\nhascommoninds(B; kwargs...) -> f::Function\n\nCheck if the ITensors or sets of indices A and B have common indices.\n\nIf only one ITensor or set of indices B is passed, return a function f such that f(A) = hascommoninds(A, B; kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"ITensorType.html#Index-Manipulations","page":"ITensor","title":"Index Manipulations","text":"","category":"section"},{"location":"ITensorType.html#ITensors.replaceind-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.replaceind","text":"replaceind[!](A::ITensor, i1::Index, i2::Index) -> ITensor\n\nReplace the Index i1 with the Index i2 in the ITensor.\n\nThe indices must have the same space (i.e. the same dimension and QNs, if applicable).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.replaceinds-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.replaceinds","text":"replaceinds(A::ITensor, inds1, inds2) -> ITensor\n\nreplaceinds!(A::ITensor, inds1, inds2)\n\nReplace the Index inds1[n] with the Index inds2[n] in the ITensor, where n runs from 1 to length(inds1) == length(inds2).\n\nThe indices must have the same space (i.e. the same dimension and QNs, if applicable).\n\nThe storage of the ITensor is not modified or copied (the output ITensor is a view of the input ITensor).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.swapind-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.swapind","text":"swapind(A::ITensor, i1::Index, i2::Index) -> ITensor\n\nswapind!(A::ITensor, i1::Index, i2::Index)\n\nSwap the Index i1 with the Index i2 in the ITensor.\n\nThe indices must have the same space (i.e. the same dimension and QNs, if applicable).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.swapinds-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"ITensors.swapinds","text":"swapinds(A::ITensor, inds1, inds2) -> ITensor\n\nswapinds!(A::ITensor, inds1, inds2)\n\nSwap the Index inds1[n] with the Index inds2[n] in the ITensor, where n runs from 1 to length(inds1) == length(inds2).\n\nThe indices must have the same space (i.e. the same dimension and QNs, if applicable).\n\nThe storage of the ITensor is not modified or copied (the output ITensor is a view of the input ITensor).\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Math-operations","page":"ITensor","title":"Math operations","text":"","category":"section"},{"location":"ITensorType.html#Base.:*-Tuple{ITensor, ITensor}","page":"ITensor","title":"Base.:*","text":"A::ITensor * B::ITensor\ncontract(A::ITensor, B::ITensor)\n\nContract ITensors A and B to obtain a new ITensor. This contraction * operator finds all matching indices common to A and B and sums over them, such that the result will have only the unique indices of A and B. To prevent indices from matching, their prime level or tags can be modified such that they no longer compare equal - for more information see the documentation on Index objects.\n\nExamples\n\ni = Index(2,\"index_i\"); j = Index(4,\"index_j\"); k = Index(3,\"index_k\")\n\nA = random_itensor(i,j)\nB = random_itensor(j,k)\nC = A * B # contract over Index j\n\nA = random_itensor(i,i')\nB = random_itensor(i,i'')\nC = A * B # contract over Index i\n\nA = random_itensor(i)\nB = random_itensor(j)\nC = A * B # outer product of A and B, no contraction\n\nA = random_itensor(i,j,k)\nB = random_itensor(k,i,j)\nC = A * B # inner product of A and B, all indices contracted\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.dag-Tuple{ITensor}","page":"ITensor","title":"ITensors.dag","text":"dag(T::ITensor; allow_alias = true)\n\nComplex conjugate the elements of the ITensor T and dagger the indices.\n\nBy default, an alias of the ITensor is returned (i.e. the output ITensor may share data with the input ITensor). If allow_alias = false, an alias is never returned.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#ITensors.directsum-Tuple{Pair{ITensor}, Pair{ITensor}, Pair{ITensor}, Vararg{Any}}","page":"ITensor","title":"ITensors.directsum","text":"directsum(A::Pair{ITensor}, B::Pair{ITensor}, ...; tags)\n\ndirectsum(output_inds, A::Pair{ITensor}, B::Pair{ITensor}, ...; tags)\n\nGiven a list of pairs of ITensors and indices, perform a partial direct sum of the tensors over the specified indices. Indices that are not specified to be summed must match between the tensors.\n\n(Note: Pair{ITensor} in Julia is short for Pair{ITensor,<:Any} which means any pair T => x where T is an ITensor.)\n\nIf all indices are specified then the operation is equivalent to creating a block diagonal tensor.\n\nReturns the ITensor representing the partial direct sum as well as the new direct summed indices. The tags of the direct summed indices are specified by the keyword arguments.\n\nOptionally, pass the new direct summed indices of the output tensor as the first argument (either a single Index or a collection), which must be proper direct sums of the input indices that are specified to be direct summed.\n\nSee Section 2.3 of https://arxiv.org/abs/1405.7786 for a definition of a partial direct sum of tensors.\n\nExamples\n\nx = Index(2, \"x\")\ni1 = Index(3, \"i1\")\nj1 = Index(4, \"j1\")\ni2 = Index(5, \"i2\")\nj2 = Index(6, \"j2\")\n\nA1 = random_itensor(x, i1)\nA2 = random_itensor(x, i2)\nS, s = directsum(A1 => i1, A2 => i2)\ndim(s) == dim(i1) + dim(i2)\n\ni1i2 = directsum(i1, i2)\nS = directsum(i1i2, A1 => i1, A2 => i2)\nhasind(S, i1i2)\n\nA3 = random_itensor(x, j1)\nS, s = directsum(A1 => i1, A2 => i2, A3 => j1)\ndim(s) == dim(i1) + dim(i2) + dim(j1)\n\nA1 = random_itensor(i1, x, j1)\nA2 = random_itensor(x, j2, i2)\nS, s = directsum(A1 => (i1, j1), A2 => (i2, j2); tags = [\"sum_i\", \"sum_j\"])\nlength(s) == 2\ndim(s[1]) == dim(i1) + dim(i2)\ndim(s[2]) == dim(j1) + dim(j2)\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Base.exp-Tuple{ITensor, Any, Any}","page":"ITensor","title":"Base.exp","text":"exp(A::ITensor, Linds=Rinds', Rinds=inds(A,plev=0); ishermitian = false)\n\nCompute the exponential of the tensor A by treating it as a matrix A_lr with the left index l running over all indices in Linds and r running over all indices in Rinds.\n\nOnly accepts index lists Linds,Rinds such that: (1) length(Linds) + length(Rinds) == length(inds(A)) (2) length(Linds) == length(Rinds) (3) For each pair of indices (Linds[n],Rinds[n]), Linds[n] and Rinds[n] represent the same Hilbert space (the same QN structure in the QN case, or just the same length in the dense case), and appear in A with opposite directions.\n\nWhen ishermitian=true the exponential of Hermitian(A_{lr}) is computed internally.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#LinearAlgebra.nullspace-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"LinearAlgebra.nullspace","text":"nullspace(T::ITensor, left_inds...; tags=\"n\", atol=1E-12, kwargs...)\n\nViewing the ITensor T as a matrix with the provided left_inds viewed as the row space and remaining indices viewed as the right indices or column space, the nullspace function computes the right null space. That is, it will return a tensor N acting on the right indices of T such that T*N is zero. The returned tensor N will also have a new index with the label \"n\" which indexes through the 'vectors' in the null space.\n\nFor example, if T has the indices i,j,k, calling N = nullspace(T,i,k) returns N with index j such that\n\n       ___       ___\n  i --|   |     |   |\n      | T |--j--| N |--n  ≈ 0\n  k --|   |     |   |\n       ---       ---\n\nThe index n can be obtained by calling n = uniqueindex(N,T)\n\nNote that the implementation of this function is subject to change in the future, in which case the precise atol value that gives a certain null space size may change in future versions of ITensor.\n\nKeyword arguments:\n\natol::Float64=1E-12 - singular values of T†*T below this value define the null space\ntags::String=\"n\" - choose the tags of the index selecting elements of the null space\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Decompositions","page":"ITensor","title":"Decompositions","text":"","category":"section"},{"location":"ITensorType.html#LinearAlgebra.svd-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"LinearAlgebra.svd","text":"svd(A::ITensor, inds::Index...; <keyword arguments>)\n\nSingular value decomposition (SVD) of an ITensor A, computed by treating the \"left indices\" provided collectively as a row index, and the remaining \"right indices\" as a column index (matricization of a tensor).\n\nThe first three return arguments are U, S, and V, such that A ≈ U * S * V.\n\nWhether or not the SVD performs a trunction depends on the keyword arguments provided.\n\nIf the left or right set of indices are empty, all input indices are put on V or U respectively. To specify an empty set of left indices, you must explicitly use svd(A, ()) (svd(A) is currently undefined).\n\nExamples\n\nComputing the SVD of an order-three ITensor, such that the indices i and k end up on U and j ends up on V\n\ni = Index(2)\nj = Index(5)\nk = Index(2)\nA = random_itensor(i, j, k)\nU, S, V = svd(A, i, k);\n@show norm(A - U * S * V) <= 10 * eps() * norm(A)\n\nThe following code will truncate the last 2 singular values, since the total number of singular values is 4. The norm of the difference with the original tensor will be the sqrt root of the sum of the squares of the singular values that get truncated.\n\ntrunc, Strunc, Vtrunc = svd(A, i, k; maxdim=2);\n@show norm(A - Utrunc * Strunc * Vtrunc) ≈ sqrt(S[3, 3]^2 + S[4, 4]^2)\n\nAlternatively we can specify that we want to truncate the weights of the singular values up to a certain cutoff, so the total error will be no larger than the cutoff.\n\nUtrunc2, Strunc2, Vtrunc2 = svd(A, i, k; cutoff=1e-10);\n@show norm(A - Utrunc2 * Strunc2 * Vtrunc2) <= 1e-10\n\nKeywords\n\nmaxdim::Int: the maximum number of singular values to keep.\nmindim::Int: the minimum number of singular values to keep.\ncutoff::Float64: set the desired truncation error of the SVD,  by default defined as the sum of the squares of the smallest singular values.\nlefttags::String = \"Link,u\": set the tags of the Index shared by U and S.\nrighttags::String = \"Link,v\": set the tags of the Index shared by S and V.\nalg::String = \"divide_and_conquer\". Options:\n\"divide_and_conquer\" - A divide-and-conquer algorithm.    LAPACK's gesdd. Fast, but may lead to some innacurate singular values    for very ill-conditioned matrices. Also may sometimes fail to converge,    leading to errors (in which case \"qr_iteration\" or \"recursive\" can be tried).\n\"qr_iteration\" - Typically slower but more accurate for very  ill-conditioned matrices compared to \"divide_and_conquer\".  LAPACK's gesvd.\n\"recursive\" - ITensor's custom svd. Very reliable, but may be slow if  high precision is needed. To get an svd of a matrix A, an  eigendecomposition of A^dagger A is used to compute U and then  a qr of A^dagger U is used to compute V. This is performed  recursively to compute small singular values.\nuse_absolute_cutoff::Bool = false: set if all probability weights below  the cutoff value should be discarded, rather than the sum of discarded  weights.\nuse_relative_cutoff::Bool = true: set if the singular values should be  normalized for the sake of truncation.\nmin_blockdim::Int = 0: for SVD of block-sparse or QN ITensors, require  that the number of singular values kept be greater than or equal to  this value when possible\n\nSee also: factorize, eigen\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#LinearAlgebra.eigen-Tuple{ITensor, Any, Any}","page":"ITensor","title":"LinearAlgebra.eigen","text":"eigen(A::ITensor[, Linds, Rinds]; <keyword arguments>)\n\nEigendecomposition of an ITensor A, computed by treating the \"left indices\" Linds provided collectively as a row index, and remaining \"right indices\" Rinds as a column index (matricization of a tensor).\n\nIf no indices are provided, pairs of primed and unprimed indices are searched for, with Linds taken to be the primed indices and Rinds taken to be the unprimed indices.\n\nThe return arguments are the eigenvalues D and eigenvectors U as tensors, such that A * U ∼ U * D (more precisely they are approximately equal up to proper replacements of indices, see the example for details).\n\nWhether or not eigen performs a trunction depends on the keyword arguments provided. Note that truncation is only well defined for positive semidefinite matrices.\n\nArguments\n\n- `maxdim::Int`: the maximum number of singular values to keep.\n- `mindim::Int`: the minimum number of singular values to keep.\n- `cutoff::Float64`: set the desired truncation error of the eigenvalues,\n   by default defined as the sum of the squares of the smallest eigenvalues.\n   For now truncation is only well defined for positive semi-definite\n   eigenspectra.\n- `ishermitian::Bool = false`: specify if the matrix is Hermitian, in which\n   case a specialized diagonalization routine will be used and it is\n   guaranteed that real eigenvalues will be returned.\n- `plev::Int = 0`: set the prime level of the Indices of `D`. Default prime\n   levels are subject to change.\n- `leftplev::Int = plev`: set the prime level of the Index unique to `D`.\n   Default prime levels are subject to change.\n- `rightplev::Int = leftplev+1`: set the prime level of the Index shared\n   by `D` and `U`. Default tags are subject to change.\n- `tags::String = \"Link,eigen\"`: set the tags of the Indices of `D`.\n   Default tags are subject to change.\n- `lefttags::String = tags`: set the tags of the Index unique to `D`.\n   Default tags are subject to change.\n- `righttags::String = tags`: set the tags of the Index shared by `D` and `U`.\n   Default tags are subject to change.\n- `use_absolute_cutoff::Bool = false`: set if all probability weights below\n   the `cutoff` value should be discarded, rather than the sum of discarded\n   weights.\n- `use_relative_cutoff::Bool = true`: set if the singular values should\n   be normalized for the sake of truncation.\n\nExamples\n\ni, j, k, l = Index(2, \"i\"), Index(2, \"j\"), Index(2, \"k\"), Index(2, \"l\")\nA = random_itensor(i, j, k, l)\nLinds = (i, k)\nRinds = (j, l)\nD, U = eigen(A, Linds, Rinds)\ndl, dr = uniqueind(D, U), commonind(D, U)\nUl = replaceinds(U, (Rinds..., dr) => (Linds..., dl))\nA * U ≈ Ul * D # true\n\nSee also: svd, factorize\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#LinearAlgebra.factorize-Tuple{ITensor, Vararg{Any}}","page":"ITensor","title":"LinearAlgebra.factorize","text":"factorize(A::ITensor, Linds::Index...; <keyword arguments>)\n\nPerform a factorization of A into ITensors L and R such that A ≈ L * R.\n\nArguments\n\northo::String = \"left\": Choose orthogonality  properties of the factorization.\n\"left\": the left factor L is an orthogonal basis  such that L * dag(prime(L, commonind(L,R))) ≈ I.\n\"right\": the right factor R forms an orthogonal basis.\n\"none\", neither of the factors form an orthogonal basis,   and in general are made as symmetrically as possible   (depending on the decomposition used).\nwhich_decomp::Union{String, Nothing} = nothing: choose what kind  of decomposition is used.\nnothing: choose the decomposition automatically based on  the other arguments. For example, when nothing is chosen and  ortho = \"left\" or \"right\", and a cutoff is provided, svd or  eigen is used depending on the provided cutoff (eigen is only  used when the cutoff is greater than 1e-12, since it has a lower  precision). When no truncation is requested qr is used for dense  ITensors and svd for block-sparse ITensors (in the future qr  will be used also for block-sparse ITensors in this case).\n\"svd\": L = U and R = S * V for ortho = \"left\", L = U * S  and R = V for ortho = \"right\", and L = U * sqrt.(S) and  R = sqrt.(S) * V for ortho = \"none\". To control which svd  algorithm is choose, use the svd_alg keyword argument. See the  documentation for svd for the supported algorithms, which are the  same as those accepted by the alg keyword argument.\n\"eigen\": L = U and R = U^dagger A where U is determined  from the eigendecompositon A A^dagger = U D U^dagger for  ortho = \"left\" (and vice versa for ortho = \"right\"). \"eigen\" is  not supported for ortho = \"none\".\n\"qr\": L=Q and R an upper-triangular matrix when  ortho = \"left\", and R = Q and L a lower-triangular matrix  when ortho = \"right\" (currently supported for dense ITensors only). In the future, other decompositions like QR (for block-sparse ITensors), polar, cholesky, LU, etc. are expected to be supported.\n\nFor truncation arguments, see: svd\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#Memory-operations","page":"ITensor","title":"Memory operations","text":"","category":"section"},{"location":"ITensorType.html#ITensors.permute-Tuple{ITensor, Any}","page":"ITensor","title":"ITensors.permute","text":"permute(T::ITensor, inds...; allow_alias = false)\n\nReturn a new ITensor T with indices permuted according to the input indices inds. The storage of the ITensor is permuted accordingly.\n\nIf called with allow_alias = true, it avoids copying data if possible. Therefore, it may return an alias of the input ITensor (an ITensor that shares the same data), such as if the permutation turns out to be trivial.\n\nBy default, allow_alias = false, and it never returns an alias of the input ITensor.\n\nExamples\n\ni = Index(2, \"index_i\"); j = Index(4, \"index_j\"); k = Index(3, \"index_k\");\nT = random_itensor(i, j, k)\n\npT_1 = permute(T, k, i, j)\npT_2 = permute(T, j, i, k)\n\npT_noalias_1 = permute(T, i, j, k)\npT_noalias_1[1, 1, 1] = 12\nT[1, 1, 1] != pT_noalias_1[1, 1, 1]\n\npT_noalias_2 = permute(T, i, j, k; allow_alias = false)\npT_noalias_2[1, 1, 1] = 12\nT[1, 1, 1] != pT_noalias_1[1, 1, 1]\n\npT_alias = permute(T, i, j, k; allow_alias = true)\npT_alias[1, 1, 1] = 12\nT[1, 1, 1] == pT_alias[1, 1, 1]\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#NDTensors.dense-Tuple{ITensor}","page":"ITensor","title":"NDTensors.dense","text":"dense(T::ITensor)\n\nMake a new ITensor where the storage is the closest Dense storage, avoiding allocating new data if possible. For example, an ITensor with Diag storage will become Dense storage, filled with zeros except for the diagonal values.\n\n\n\n\n\n","category":"method"},{"location":"ITensorType.html#NDTensors.denseblocks-Tuple{ITensor}","page":"ITensor","title":"NDTensors.denseblocks","text":"denseblocks(T::ITensor)\n\nMake a new ITensor where any blocks which have a sparse format, such as diagonal sparsity, are made dense while still preserving the outer block-sparse structure. This method avoids allocating new data if possible.\n\nFor example, an ITensor with DiagBlockSparse storage will have BlockSparse storage afterwards.\n\n\n\n\n\n","category":"method"},{"location":"faq/JuliaPkg.html#Julia-Package-Manager-Frequently-Asked-Questions","page":"Julia Package Manager FAQs","title":"Julia Package Manager Frequently Asked Questions","text":"","category":"section"},{"location":"faq/JuliaPkg.html#What-if-I-can't-upgrade-ITensors.jl-to-the-latest-version?","page":"Julia Package Manager FAQs","title":"What if I can't upgrade ITensors.jl to the latest version?","text":"","category":"section"},{"location":"faq/JuliaPkg.html","page":"Julia Package Manager FAQs","title":"Julia Package Manager FAQs","text":"Sometimes you may find that doing ] update ITensors or equivalently doing ] up ITensors within Julia package manager mode doesn't result in the ITensors package actually being upgraded. You may see that the current version you have remains stuck to a version that is lower than the latest one which you can check here.","category":"page"},{"location":"faq/JuliaPkg.html","page":"Julia Package Manager FAQs","title":"Julia Package Manager FAQs","text":"What is most likely going on is that you have other packages installed which are blocking ITensors from being updated.","category":"page"},{"location":"faq/JuliaPkg.html","page":"Julia Package Manager FAQs","title":"Julia Package Manager FAQs","text":"To get more information into which packages may be doing this, and what versions they are requiring, you can do the following. First look up the latest version of ITensors.jl. Let's say for this example that it is v0.3.0.","category":"page"},{"location":"faq/JuliaPkg.html","page":"Julia Package Manager FAQs","title":"Julia Package Manager FAQs","text":"Next, input the following command while in package manager mode:","category":"page"},{"location":"faq/JuliaPkg.html","page":"Julia Package Manager FAQs","title":"Julia Package Manager FAQs","text":"julia> ]\npkg> add ITensors@v0.3.0","category":"page"},{"location":"faq/JuliaPkg.html","page":"Julia Package Manager FAQs","title":"Julia Package Manager FAQs","text":"If the package manager cannot update to this version, it will list all of the other packages that are blocking this from happening and give information about why. To go into a little more depth, each package has a compatibility or \"compat\" entry in its Project.toml file which says which versions of the ITensors package it is compatible with. If these versions do not include the latest one, perhaps because the package has not been updated, then it can block the ITensors package from being updated on your system.","category":"page"},{"location":"faq/JuliaPkg.html","page":"Julia Package Manager FAQs","title":"Julia Package Manager FAQs","text":"Generally the solution is to just update each of these packages, then try again to update ITensors. If that does not work, then check the following","category":"page"},{"location":"faq/JuliaPkg.html","page":"Julia Package Manager FAQs","title":"Julia Package Manager FAQs","text":"Are any of the blocking packages in \"dev mode\" meaning you called dev PackageName on them in the past? Try doing free PackageName if so to bring them out of dev mode.\nAre any of the blocking packages unregistered packages that were installed through a GitHub repo link? If so, you may need to do something like add https://github.com/Org/PackageName#main to force update that package to the latest code available on its main branch.","category":"page"},{"location":"faq/JuliaPkg.html","page":"Julia Package Manager FAQs","title":"Julia Package Manager FAQs","text":"If you still can't get the ITensors package update, feel free to post a question or contact us for help.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#Upgrade-guide","page":"Upgrading from 0.1 to 0.2","title":"Upgrade guide","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html#Upgrading-from-ITensors.jl-0.1-to-0.2","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from ITensors.jl 0.1 to 0.2","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"The main breaking changes in ITensor.jl v0.2 involve changes to the ITensor, IndexSet, and IndexVal types. Most user code should be fine, but see below for more details.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"In addition, we have moved development of NDTensors.jl into ITensors.jl to simplify the development process until NDTensors is more stable and can be a standalone package. Again, see below for more details.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"For a more comprehensive list of changes, see the commit history on Github.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"If you have issues upgrading, please reach out by raising an issue on Github or asking a question on the ITensor support forum.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Also make sure to run your code with julia --depwarn=yes to see warnings about function names and interfaces that have been deprecated and will be removed in v0.3 of ITensors.jl (these are not listed here).","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#Major-design-changes:-changes-to-the-ITensor,-IndexSet,-and-IndexVal-types","page":"Upgrading from 0.1 to 0.2","title":"Major design changes: changes to the ITensor, IndexSet, and IndexVal types","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html#Changes-to-the-ITensor-type","page":"Upgrading from 0.1 to 0.2","title":"Changes to the ITensor type","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html#Removal-of-tensor-order-type-parameter","page":"Upgrading from 0.1 to 0.2","title":"Removal of tensor order type parameter","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"The tensor order type paramater has been removed from the ITensor type, so you can no longer write ITensor{3} to specify an order 3 ITensor (PR #591). Code that uses the ITensor order type parameter will now lead to the following error:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> i = Index(2)\n(dim=2|id=588)\n\njulia> ITensor{2}(i', i)\nERROR: TypeError: in Type{...} expression, expected UnionAll, got Type{ITensor}\nStacktrace:\n [1] top-level scope\n   @ REPL[27]:1","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Simply remove the type parameter:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> ITensor(i', i)\nITensor ord=2 (dim=2|id=913)' (dim=2|id=913)\nITensors.NDTensors.EmptyStorage{ITensors.NDTensors.EmptyNumber, ITensors.NDTensors.Dense{ITensors.NDTensors.EmptyNumber, Vector{ITensors.NDTensors.EmptyNumber}}}","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Pro tip: from the command line, you can replace all examples like that with:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"find . -type f -iname \"*.jl\" -exec sed -i 's/ITensor{.*}/ITensor/g' \"{}\" +","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Of course, make sure to back up your code before running this!","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Additionally, a common code pattern may be using the type parameter for dispatch:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"using ITensors\n\nfunction mynorm(A::ITensor{N}) where {N}\n  return norm(A)^N\nend\n\nfunction mynorm(A::ITensor{1})\n  return norm(A)\nend\n\nfunction mynorm(A::ITensor{2})\n  return norm(A)^2\nend","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Instead, you can use an if-statement:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"function mynormN(A::ITensor)\n  return norm(A)^order(A)\nend\n\nfunction mynorm1(A::ITensor)\n  return norm(A)\nend\n\nfunction mynorm2(A::ITensor)\n  return norm(A)^2\nend\n\nfunction mynorm(A::ITensor)\n  return if order(A) == 1\n    mynorm1(A)\n  elseif order(A) == 2\n    mynorm2(A)\n  else\n    return mynormN(A)\n  end\nend","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Alternatively, you can use the Order type to dispatch on the ITensor order as follows:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"function mynorm(::Order{N}, A::ITensor) where {N}\n  return norm(A)^N\nend\n\nfunction mynorm(::Order{1}, A::ITensor)\n  return norm(A)\nend\n\nfunction mynorm(::Order{2}, A::ITensor)\n  return norm(A)^2\nend\n\nfunction mynorm(A::ITensor)\n  return mynorm(Order(A), A)\nend","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Order(A::ITensor) returns the order of the ITensor (like order(A::ITensor)), however as a type that can be dispatched on. Note that it is not type stable, so there will be a small runtime overhead for doing this.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#Change-to-storage-type-of-Index-collection-in-ITensor","page":"Upgrading from 0.1 to 0.2","title":"Change to storage type of Index collection in ITensor","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"ITensors now store a Tuple of Index instead of an IndexSet (PR #626). Therefore, calling inds on an ITensor will now just return a Tuple:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> i = Index(2)\n(dim=2|id=770)\n\njulia> j = Index(3)\n(dim=3|id=272)\n\njulia> A = random_itensor(i, j)\nITensor ord=2 (dim=2|id=770) (dim=3|id=272)\nITensors.NDTensors.Dense{Float64, Vector{Float64}}\n\njulia> inds(A)\n((dim=2|id=770), (dim=3|id=272))","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"while before it returned an IndexSet (in fact, the IndexSet type has been removed, see below for details). In general, this should not affect user code, since a Tuple of Index should have all of the same functions defined for it that IndexSet did. If you find this is not the case, please raise an issue on Github or on the ITensor support forum.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#ITensor-type-now-directly-wraps-a-Tensor","page":"Upgrading from 0.1 to 0.2","title":"ITensor type now directly wraps a Tensor","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"The ITensor type no longer has separate field inds and store, just a single field tensor (PR #626). In general you should not be accessing the fields directly, instead you should be using the functions inds(A::ITensor) and storage(A::ITensor), so this should not affect most code. However, in case you have code like:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"i = Index(2)\nA = random_itensor(i)\nA.inds","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"this will error in v0.2 with:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> A.inds\nERROR: type ITensor has no field inds\nStacktrace:\n [1] getproperty(x::ITensor, f::Symbol)\n   @ Base ./Base.jl:33\n [2] top-level scope\n   @ REPL[43]:1","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"and you should change it to:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"inds(A)","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#Changes-to-the-ITensor-constructors","page":"Upgrading from 0.1 to 0.2","title":"Changes to the ITensor constructors","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html#Plain-ITensor-constructors-now-return-ITensors-with-EmptyStorage-storage","page":"Upgrading from 0.1 to 0.2","title":"Plain ITensor constructors now return ITensors with EmptyStorage storage","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"ITensor constructors from collections of Index, such as ITensor(i, j, k), now return an ITensor with EmptyStorage (previously called Empty) storage instead of Dense or BlockSparse storage filled with 0 values. Most operations should still work that worked previously, but please contact us if there are issues (PR #641).","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"For example:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> i = Index(2)\n(dim=2|id=346)\n\njulia> A = ITensor(i', dag(i))\nITensor ord=2 (dim=2|id=346)' (dim=2|id=346)\nITensors.NDTensors.EmptyStorage{ITensors.NDTensors.EmptyNumber, ITensors.NDTensors.Dense{ITensors.NDTensors.EmptyNumber, Vector{ITensors.NDTensors.EmptyNumber}}}\n\njulia> A' * A\nITensor ord=2 (dim=2|id=346)'' (dim=2|id=346)\nITensors.NDTensors.EmptyStorage{ITensors.NDTensors.EmptyNumber, ITensors.NDTensors.Dense{ITensors.NDTensors.EmptyNumber, Vector{ITensors.NDTensors.EmptyNumber}}}","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"so now contracting two EmptyStorage ITensors returns another EmptyStorage ITensor. You can allocate the storage by setting elements of the ITensor:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> A[i' => 1, i => 1] = 0.0\n0.0\n\njulia> @show A;\nA = ITensor ord=2\nDim 1: (dim=2|id=346)'\nDim 2: (dim=2|id=346)\nITensors.NDTensors.Dense{Float64, Vector{Float64}}\n 2×2\n 0.0  0.0\n 0.0  0.0","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Additionally, it will take on the element type of the first value set:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> A = ITensor(i', dag(i))\nITensor ord=2 (dim=2|id=346)' (dim=2|id=346)\nITensors.NDTensors.EmptyStorage{ITensors.NDTensors.EmptyNumber, ITensors.NDTensors.Dense{ITensors.NDTensors.EmptyNumber, Vector{ITensors.NDTensors.EmptyNumber}}}\n\njulia> A[i' => 1, i => 1] = 1.0 + 0.0im\n1.0 + 0.0im\n\njulia> @show A;\nA = ITensor ord=2\nDim 1: (dim=2|id=346)'\nDim 2: (dim=2|id=346)\nITensors.NDTensors.Dense{ComplexF64, Vector{ComplexF64}}\n 2×2\n 1.0 + 0.0im  0.0 + 0.0im\n 0.0 + 0.0im  0.0 + 0.0im","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"If you have issues upgrading, please let us know.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#Slight-change-to-automatic-conversion-of-element-type-when-constructing-ITensor-from-Array","page":"Upgrading from 0.1 to 0.2","title":"Slight change to automatic conversion of element type when constructing ITensor from Array","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"ITensor constructors from Array now only convert to floating point for Array{Int} and Array{Complex{Int}}. That same conversion is added for QN ITensor constructors to be consistent with non-QN versions (PR #620). Previously it tried to convert arrays of any element type to the closest floating point type with Julia's float function. This should not affect most user code.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#Changes-to-the-IndexSet-type","page":"Upgrading from 0.1 to 0.2","title":"Changes to the IndexSet type","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"The IndexSet type has been removed in favor of Julia's Tuple and Vector types (PR #626). ITensors now contain a Tuple of Index, while set operations like commoninds that used to return IndexSet now return a Vector of Index:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> i = Index(2)\n(dim=2|id=320)\n\njulia> A = random_itensor(i', i)\nITensor ord=2 (dim=2|id=320)' (dim=2|id=320)\nITensors.NDTensors.Dense{Float64, Vector{Float64}}\n\njulia> inds(A) # Previously returned IndexSet, now returns Tuple\n((dim=2|id=320)', (dim=2|id=320))\n\njulia> commoninds(A', A) # Previously returned IndexSet, now returns Vector\n1-element Vector{Index{Int64}}:\n (dim=2|id=320)'","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"To help with upgrading code, IndexSet{IndexT} has been redefined as a type alias for Vector{IndexT<:Index} (which is subject to change to some other collection of indices, and likely will be removed in ITensors v0.3). Therefore it no longer has a type parameter for the number of indices, similar to the change to the ITensor type. If you were using the plain IndexSet type, code should generally still work properly. However, if you were using the type parameters of IndexSet, such as:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"function myorder2(is::IndexSet{N}) where {N}\n  return N^2\nend","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"then you will need to remove the type parameter and rewrite your code generically to accept Tuple or Vector, such as:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"function myorder2(is)\n  return length(is)^2\nend","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"In general you should be able to just remove usages of IndexSet in your code, and can just use Tuple or Vector of Index instead, such as change is = IndexSet(i, j, k) to is = (i, j, k) or is = [i, j, k]. Priming, tagging, and set operations now work generically on those types. If you see issues with upgrading your code, please let us know.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#Changes-to-the-IndexVal-type","page":"Upgrading from 0.1 to 0.2","title":"Changes to the IndexVal type","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Similar to the removal of IndexSet, we have also removed the IndexVal type (PR #665). Now, all use cases of IndexVal can be replaced by using Julia's Pair type, for example instead of:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"i = Index(2)\nIndexVal(i, 2)","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"use:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"i = Index(2)\ni => 2\n# Or:\nPair(i, 2)","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Note that we have made IndexVal{IndexT} an alias for Pair{IndexT,Int}, so code using IndexVal such as IndexVal(i, 2) should generally still work. However, we encourage users to change from IndexVal(i, 2) to i => 2.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#NDTensors.jl-package-now-being-developed-internally-within-ITensors.jl","page":"Upgrading from 0.1 to 0.2","title":"NDTensors.jl package now being developed internally within ITensors.jl","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"The NDTensors module has been moved into the ITensors package, so ITensors no longer depends on the standalone NDTensors package. This should only effect users who were using both NDTensors and ITensors seperately. If you want to use the latest NDTensors library, you should do using ITensors.NDTensors instead of using NDTensors, and will need to install ITensors with using Pkg; Pkg.add(\"ITensors\") in order to use the latest versions of NDTensors. Note the current NDTensors.jl package will still exist, but for now developmentof NDTensors will occur within ITensors.jl (PR #650).","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#Miscellaneous-breaking-changes","page":"Upgrading from 0.1 to 0.2","title":"Miscellaneous breaking changes","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html#state-function-renamed-val,-state-given-a-new-more-general-definition","page":"Upgrading from 0.1 to 0.2","title":"state function renamed val, state given a new more general definition","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Rename the state functions currently defined for various site types to val for mapping a string name for an index to an index value (used in ITensor indexing and MPS construction). state functions now return single-index ITensors representing various single-site states (PR #664). So now to get an Index value from a string, you use:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"N = 10\ns = siteinds(\"S=1/2\", N)\nval(s[1], \"Up\") == 1\nval(s[1], \"Dn\") == 2","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"state now returns an ITensor corresponding to the state with that value as the only nonzero element:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> @show state(s[1], \"Up\");\nstate(s[1], \"Up\") = ITensor ord=1\nDim 1: (dim=2|id=597|\"S=1/2,Site,n=1\")\nITensors.NDTensors.Dense{Float64, Vector{Float64}}\n 2-element\n 1.0\n 0.0\n\njulia> @show state(s[1], \"Dn\");\nstate(s[1], \"Dn\") = ITensor ord=1\nDim 1: (dim=2|id=597|\"S=1/2,Site,n=1\")\nITensors.NDTensors.Dense{Float64, Vector{Float64}}\n 2-element\n 0.0\n 1.0","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"which allows for more general states to be defined, such as:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> @show state(s[1], \"X+\");\nstate(s[1], \"X+\") = ITensor ord=1\nDim 1: (dim=2|id=597|\"S=1/2,Site,n=1\")\nITensors.NDTensors.Dense{Float64, Vector{Float64}}\n 2-element\n 0.7071067811865475\n 0.7071067811865475\n\njulia> @show state(s[1], \"X-\");\nstate(s[1], \"X-\") = ITensor ord=1\nDim 1: (dim=2|id=597|\"S=1/2,Site,n=1\")\nITensors.NDTensors.Dense{Float64, Vector{Float64}}\n 2-element\n  0.7071067811865475\n -0.7071067811865475","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"which will be used for making more general MPS product states.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"This should not affect end users in general, besides ones who had customized the previous state function, such as with overloads like:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"ITensors.state(::SiteType\"My_S=1/2\", ::StateName\"Up\") = 1\nITensors.state(::SiteType\"My_S=1/2\", ::StateName\"Dn\") = 2","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"which should be changed now to:","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"ITensors.val(::SiteType\"My_S=1/2\", ::StateName\"Up\") = 1\nITensors.val(::SiteType\"My_S=1/2\", ::StateName\"Dn\") = 2","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#\"Qubit\"-site-type-QN-convention-change","page":"Upgrading from 0.1 to 0.2","title":"\"Qubit\" site type QN convention change","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"The QN convention of the \"Qubit\" site type is changed to track the total number of 1 bits instead of the net number of 1 bits vs 0 bits (i.e. change the QN from +1/-1 to 0/1) (PR #676).","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> s = siteinds(\"Qubit\", 4; conserve_number=true)\n4-element Vector{Index{Vector{Pair{QN, Int64}}}}:\n (dim=2|id=925|\"Qubit,Site,n=1\") <Out>\n 1: QN(\"Number\",0) => 1\n 2: QN(\"Number\",1) => 1\n (dim=2|id=799|\"Qubit,Site,n=2\") <Out>\n 1: QN(\"Number\",0) => 1\n 2: QN(\"Number\",1) => 1\n (dim=2|id=8|\"Qubit,Site,n=3\") <Out>\n 1: QN(\"Number\",0) => 1\n 2: QN(\"Number\",1) => 1\n (dim=2|id=385|\"Qubit,Site,n=4\") <Out>\n 1: QN(\"Number\",0) => 1\n 2: QN(\"Number\",1) => 1","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"Before it was +1/-1 like \"S=1/2\":","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"julia> s = siteinds(\"S=1/2\", 4; conserve_sz=true)\n4-element Vector{Index{Vector{Pair{QN, Int64}}}}:\n (dim=2|id=364|\"S=1/2,Site,n=1\") <Out>\n 1: QN(\"Sz\",1) => 1\n 2: QN(\"Sz\",-1) => 1\n (dim=2|id=823|\"S=1/2,Site,n=2\") <Out>\n 1: QN(\"Sz\",1) => 1\n 2: QN(\"Sz\",-1) => 1\n (dim=2|id=295|\"S=1/2,Site,n=3\") <Out>\n 1: QN(\"Sz\",1) => 1\n 2: QN(\"Sz\",-1) => 1\n (dim=2|id=810|\"S=1/2,Site,n=4\") <Out>\n 1: QN(\"Sz\",1) => 1\n 2: QN(\"Sz\",-1) => 1","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"This shouldn't affect end users in general. The new convention is a bit more intuitive since the quantum number can be thought of as counting the total number of 1 bits in the state, though the conventions can be mapped to each other with a constant.","category":"page"},{"location":"UpgradeGuide_0.1_to_0.2.html#maxlinkdim-for-MPS/MPO-with-no-indices","page":"Upgrading from 0.1 to 0.2","title":"maxlinkdim for MPS/MPO with no indices","text":"","category":"section"},{"location":"UpgradeGuide_0.1_to_0.2.html","page":"Upgrading from 0.1 to 0.2","title":"Upgrading from 0.1 to 0.2","text":"maxlinkdim(::MPS/MPO) returns a minimum of 1 (previously it returned 0 for MPS/MPO without and link indices) (PR #663).","category":"page"},{"location":"index.html#ITensors.jl","page":"Introduction","title":"ITensors.jl","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ITensor is a library for rapidly creating correct and efficient tensor network algorithms.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Documentation Citation\n(Image: Stable) (Image: Dev) (Image: SciPost) (Image: arXiv)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Version Download Statistics Style Guide License\n(Image: version) (Image: ITensor Downloads) (Image: Code Style: Blue) (Image: license)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The source code for ITensors.jl can be found on Github.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Additional documentation can be found on the ITensor website itensor.org.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"An ITensor is a tensor whose interface is independent of its memory layout. ITensor indices are objects which carry extra information and which 'recognize' each other (compare equal to each other).","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The ITensorMPS.jl library includes composable and extensible algorithms for optimizing and transforming tensor networks, such as matrix product state and matrix product operators, such as the DMRG algorithm. If you are looking for information on running finite MPS/MPO calculations such as DMRG, take a look at the ITensorMPS.jl documentation.","category":"page"},{"location":"index.html#Support","page":"Introduction","title":"Support","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"<img class=\"display-light-only\" src=\"assets/CCQ.png\" width=\"20%\" alt=\"Flatiron Center for Computational Quantum Physics logo.\"/>\n<img class=\"display-dark-only\" src=\"assets/CCQ-dark.png\" width=\"20%\" alt=\"Flatiron Center for Computational Quantum Physics logo.\"/>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ITensors.jl is supported by the Flatiron Institute, a division of the Simons Foundation.","category":"page"},{"location":"index.html#News","page":"Introduction","title":"News","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"March 26, 2025: ITensors.jl v0.9 has been released. This is a minor breaking change since the optimal_contraction_sequence function now passes to the optimaltree function from TensorOperations.jl. The TensorOperations package therefore needs to be loaded in order for optimal_contraction_sequence to be used or if the flag ITensors.enable_contraction_sequence_optimization() is switched on.\nMarch 22, 2025: As part of the latest release of ITensors.jl (v0.8.3), all documentation related to MPS/MPO functionality has been moved to the ITensorMPS.jl documentation.\nFebruary 22, 2025: Please note that there were issues installing the latest version of ITensors.jl (ITensors.jl v0.8) in older versions of Julia v1.10 and v1.11 (https://github.com/ITensor/ITensors.jl/issues/1618, https://itensor.discourse.group/t/typeparameteraccessors-not-found-error-on-julia-v-1-10-0/2260). This issue has been fixed in NDTensors.jl v0.4.4, so please try updating your packages if you are using older versions of Julia v1.10 or v1.11 and running into issues installing ITensors.jl.\nFebruary 3, 2025: ITensors.jl v0.8 has been released. This release should not be breaking to the average user using documented features of the library. This removes internal submodules that held experimental code for rewriting the internals of NDTensors.jl/ITensors.jl, which have now been turned into separate packages for future development. It is marked as breaking since ITensorMPS.jl was making use of some of that experimental code, and will be updated accordingly. Also note that it fixes an issue that existed in some more recent versions of NDTensors.jl v0.3/ITensors.jl v0.7 where loading ITensors.jl in combination with some packages like LinearMaps.jl caused very long load/compile times (https://itensor.discourse.group/t/linearmaps-and-itensors-incompatibility/2216), so if you are seeing that issue when using ITensors.jl v0.7 you should upgrade to this version.\nOctober 25, 2024: ITensors.jl v0.7 has been released. This is a major breaking change, since all of the MPS/MPO functionality from this package has been moved to ITensorMPS.jl, along with all of the functionality of ITensorTDVP.jl. If you want to use MPS/MPO types and related functionality, such as MPS, MPO, dmrg, siteinds, OpSum, op, etc. you now must install and load the ITensorMPS.jl package. Additionally, if you are using ITensorTDVP.jl in your code, please change using ITensorTDVP to using ITensorMPS. ITensorMPS.jl has all of the same functionality as ITensorTDVP.jl, and ITensorTDVP.jl will be deprecated in favor of ITensorMPS.jl. Note: If you are using ITensors.compile, you must now install and load the ITensorMPS.jl package in order to trigger it to load properly, since it relies on running MPS/MPO functionality as example code for Julia to compile.\nMay 9, 2024: A new package ITensorMPS.jl has been released. We plan to move all of the MPS/MPO functionality in ITensors.jl to ITensorMPS.jl. For now, ITensorMPS.jl just re-exports the MPS/MPO functionality of ITensors.jl (as well as of ITensorTDVP.jl), such as dmrg, siteinds, MPS, MPO, etc. To prepare for the change over to ITensorMPS.jl, please change using ITensors to using ITensors, ITensorMPS in any code that makes use of MPS/MPO functionality, and if you are using ITensorTDVP.jl change using ITensorTDVP to using ITensorMPS in your code.\nMay 8, 2024: ITensors.jl v0.6 has been released. This version deletes the experimental \"combine-contract\" contraction backend, which was enabled by ITensors.enable_combine_contract(). This feature enabled performing ITensor contractions by first combining indices and then performing contractions as matrix multiplications, which potentially could lead to speedups for certain contractions involving higher-order QN-conserving tensors. However, the speedups weren't consistent with the current implementation, and this feature will be incorporated into the library in a more systematic way when we release our new non-abelian symmetric tensor backend.\nMay 2, 2024: ITensors.jl v0.5 has been released. This version removes PackageCompiler.jl as a dependency and moves the package compilation functionality into a package extension. In order to use the ITensors.compile() function going forward, you need to install the PackageCompiler.jl package with using Pkg: Pkg; Pkg.add(\"PackageCompiler\") and put using PackageCompiler together with using ITensors in your code.\nApril 16, 2024: ITensors.jl v0.4 has been released. This version removes HDF5.jl as a dependency and moves the HDF5 read and write functions for ITensor, MPS, MPO, and other associated types into a package extension. To enable ITensor HDF5 features, install the HDF5.jl package with using Pkg: Pkg; Pkg.add(\"HDF5\") and put using HDF5 together with using ITensors in your code. Other recent changes include support for multiple GPU backends using package extensions.\nMarch 25, 2022: ITensors.jl v0.3 has been released. The main breaking change is that we no longer support versions of Julia below 1.6. Julia 1.6 is the long term support version of Julia (LTS), which means that going forward versions below Julia 1.6 won't be as well supported with bug fixes and improvements. Additionally, Julia 1.6 introduced many improvements including syntax improvements that we would like to start using with ITensors.jl, which becomes challenging if we try to support Julia versions below 1.6. See here and here for some nice summaries of the Julia 1.6 release.\nJun 09, 2021: ITensors.jl v0.2 has been released, with a few breaking changes as well as a variety of bug fixes","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"and new features. Take a look at the upgrade guide for help upgrading your code.","category":"page"},{"location":"index.html#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The ITensors package can be installed with the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"~ julia","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"julia> ]\n\npkg> add ITensors","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Or, equivalently, via the Pkg API:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"julia> import Pkg; Pkg.add(\"ITensors\")","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Please note that right now, ITensors.jl requires that you use Julia v1.3 or later (since ITensors.jl relies on a feature that was introduced in Julia v1.3).","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"We recommend using ITensors.jl with Intel MKL in order to get the best possible performance. If you have not done so already, you can replace your current BLAS and LAPACK implementation with MKL by using the MKL.jl package. Please follow the instructions here.","category":"page"},{"location":"index.html#Documentation","page":"Introduction","title":"Documentation","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"LATEST – documentation of the latest version.","category":"page"},{"location":"index.html#Citation","page":"Introduction","title":"Citation","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"If you use ITensor in your work, please cite the ITensor Paper:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"@article{ITensor,\n\ttitle={{The ITensor Software Library for Tensor Network Calculations}},\n\tauthor={Matthew Fishman and Steven R. White and E. Miles Stoudenmire},\n\tjournal={SciPost Phys. Codebases},\n\tpages={4},\n\tyear={2022},\n\tpublisher={SciPost},\n\tdoi={10.21468/SciPostPhysCodeb.4},\n\turl={https://scipost.org/10.21468/SciPostPhysCodeb.4},\n}","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"and associated \"Codebase Release\" for the version you have used. The current one is","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"@article{ITensor-r0.3,\n\ttitle={{Codebase release 0.3 for ITensor}},\n\tauthor={Matthew Fishman and Steven R. White and E. Miles Stoudenmire},\n\tjournal={SciPost Phys. Codebases},\n\tpages={4-r0.3},\n\tyear={2022},\n\tpublisher={SciPost},\n\tdoi={10.21468/SciPostPhysCodeb.4-r0.3},\n\turl={https://scipost.org/10.21468/SciPostPhysCodeb.4-r0.3},\n}","category":"page"},{"location":"index.html#ITensor-Code-Samples","page":"Introduction","title":"ITensor Code Samples","text":"","category":"section"},{"location":"index.html#Basic-Overview","page":"Introduction","title":"Basic Overview","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ITensor construction, setting of elements, contraction, and addition. Before constructing an ITensor, one constructs Index objects representing tensor indices.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using ITensors\nlet\n  i = Index(3)\n  j = Index(5)\n  k = Index(2)\n  l = Index(7)\n\n  A = ITensor(i,j,k)\n  B = ITensor(j,l)\n\n  # Set elements of A\n  A[i=>1,j=>1,k=>1] = 11.1\n  A[i=>2,j=>1,k=>2] = -21.2\n  A[k=>1,i=>3,j=>1] = 31.1  # can provide Index values in any order\n  # ...\n\n  # Contract over shared index j\n  C = A * B\n\n  @show hasinds(C,i,k,l) # = true\n\n  D = random_itensor(k,j,i) # ITensor with random elements\n\n  # Add two ITensors\n  # must have same set of indices\n  # but can be in any order\n  R = A + D\n\n  nothing\nend\n\n# output\n\nhasinds(C, i, k, l) = true","category":"page"},{"location":"index.html#Singular-Value-Decomposition-(SVD)-of-a-Matrix","page":"Introduction","title":"Singular Value Decomposition (SVD) of a Matrix","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"In this example, we create a random 10x20 matrix and compute its SVD. The resulting factors can be simply multiplied back together using the ITensor * operation, which automatically recognizes the matching indices between U and S, and between S and V and contracts (sums over) them.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using ITensors\nlet\n  i = Index(10)           # index of dimension 10\n  j = Index(20)           # index of dimension 20\n  M = random_itensor(i,j)  # random matrix, indices i,j\n  U,S,V = svd(M,i)        # compute SVD with i as row index\n  @show M ≈ U*S*V         # = true\n\n  nothing\nend\n\n# output\n\nM ≈ U * S * V = true","category":"page"},{"location":"index.html#Singular-Value-Decomposition-(SVD)-of-a-Tensor","page":"Introduction","title":"Singular Value Decomposition (SVD) of a Tensor","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"In this example, we create a random 4x4x4x4 tensor and compute its SVD, temporarily treating the indices i and k together as the \"row\" index and j and l as the \"column\" index for the purposes of the SVD. The resulting factors can be simply multiplied back together using the ITensor * operation, which automatically recognizes the matching indices between U and S, and between S and V and contracts (sums over) them.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using ITensors\nlet\n  i = Index(4,\"i\")\n  j = Index(4,\"j\")\n  k = Index(4,\"k\")\n  l = Index(4,\"l\")\n  T = random_itensor(i,j,k,l)\n  U,S,V = svd(T,i,k)   # compute SVD with (i,k) as row indices (indices of U)\n  @show hasinds(U,i,k) # = true\n  @show hasinds(V,j,l) # = true\n  @show T ≈ U*S*V      # = true\n\n  nothing\nend\n\n# output\n\nhasinds(U, i, k) = true\nhasinds(V, j, l) = true\nT ≈ U * S * V = true","category":"page"},{"location":"index.html#Tensor-Indices:-Tags-and-Prime-Levels","page":"Introduction","title":"Tensor Indices: Tags and Prime Levels","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Before making an ITensor, you have to define its indices. Tensor Index objects carry extra information beyond just their dimension.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"All Index objects carry a permanent, immutable id number which is determined when it is constructed, and allow it to be matched (compare equal) with copies of itself.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Additionally, an Index can have up to four tag strings, and an integer primelevel. If two Index objects have different tags or different prime levels, they do not compare equal even if they have the same id.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Tags are also useful for identifying Index objects when printing tensors, and for performing certain Index manipulations (e.g. priming indices having certain sets of tags).","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using ITensors\nlet\n  i = Index(3)     # Index of dimension 3\n  @show dim(i)     # = 3\n  @show id(i)      # = 0x5d28aa559dd13001 or similar\n\n  ci = copy(i)\n  @show ci == i    # = true\n\n  j = Index(5,\"j\") # Index with a tag \"j\"\n\n  @show j == i     # = false\n\n  s = Index(2,\"n=1,Site\") # Index with two tags,\n                          # \"Site\" and \"n=1\"\n  @show hastags(s,\"Site\") # = true\n  @show hastags(s,\"n=1\")  # = true\n\n  i1 = prime(i) # i1 has a \"prime level\" of 1\n                # but otherwise same properties as i\n  @show i1 == i # = false, prime levels do not match\n\n  nothing\nend\n\n# output\n\ndim(i) = 3\nid(i) = 0x5d28aa559dd13001\nci == i = true\nj == i = false\nhastags(s, \"Site\") = true\nhastags(s, \"n=1\") = true\ni1 == i = false","category":"page"},{"location":"ContractionSequenceOptimization.html#Contraction-sequence-optimization","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"","category":"section"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"When contracting a tensor network, the sequence of contraction makes a big difference in the computational cost. However, the complexity of determining the optimal sequence grows exponentially with the number of tensors, but there are many heuristic algorithms available for computing optimal sequences for small networks[1][2][3][4][5][6]. ITensors.jl imports functionality from TensorOperations.jl for helping you find the optimal contraction sequence for small tensor network, as we will show below.","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"[1]: Faster identification of optimal contraction sequences for tensor networks","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"[2]: Improving the efficiency of variational tensor network algorithms","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"[3]: Simulating quantum computation by contracting tensor networks","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"[4]: Towards a polynomial algorithm for optimal contraction sequence of tensor networks from trees","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"[5]: Algorithms for Tensor Network Contraction Ordering","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"[6]: Hyper-optimized tensor network contraction","category":"page"},{"location":"ContractionSequenceOptimization.html#Functions","page":"Contraction sequence optimization","title":"Functions","text":"","category":"section"},{"location":"ContractionSequenceOptimization.html#ITensors.optimal_contraction_sequence","page":"Contraction sequence optimization","title":"ITensors.optimal_contraction_sequence","text":"optimal_contraction_sequence(T)\n\nReturns a contraction sequence for contracting the tensors T. The sequence is generally optimal and is found via the optimaltree function in TensorOperations.jl which must be loaded.\n\n\n\n\n\n","category":"function"},{"location":"ContractionSequenceOptimization.html#ITensors.contraction_cost","page":"Contraction sequence optimization","title":"ITensors.contraction_cost","text":"contraction_cost(A; sequence)\n\nReturn the cost of contracting the collection of ITensors according to the specified sequence, where the cost is measured in the number of floating point operations that would need to be performed to contract dense tensors of the dimensions specified by the indices of the tensors (so for now, sparsity is ignored in computing the costs). Pairwise costs are returned in a vector (contracting N tensors requires N-1 pairwise contractions). You can use sum(contraction_cost(A; sequence)) to get the total cost of the contraction.\n\nIf no sequence is specified, left associative contraction is used, in other words the sequence is equivalent to [[[[1, 2], 3], 4], …].\n\n\n\n\n\n","category":"function"},{"location":"ContractionSequenceOptimization.html#NDTensors.contract","page":"Contraction sequence optimization","title":"NDTensors.contract","text":"*(As::ITensor...; sequence = default_sequence(), kwargs...)\n*(As::Vector{<: ITensor}; sequence = default_sequence(), kwargs...)\ncontract(As::ITensor...; sequence = default_sequence(), kwargs...)\n\nContract the set of ITensors according to the contraction sequence.\n\nThe default sequence is \"automatic\" if ITensors.using_contraction_sequence_optimization() is true, otherwise it is \"left_associative\" (the ITensors are contracted from left to right).\n\nYou can change the default with ITensors.enable_contraction_sequence_optimization() and ITensors.disable_contraction_sequence_optimization().\n\nFor a custom sequence, the sequence should be provided as a binary tree where the leaves are integers n specifying the ITensor As[n] and branches are accessed by indexing with 1 or 2, i.e. sequence = Any[Any[1, 3], Any[2, 4]].\n\n\n\n\n\n","category":"function"},{"location":"ContractionSequenceOptimization.html#Examples","page":"Contraction sequence optimization","title":"Examples","text":"","category":"section"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"In the following example we show how to compute the contraction sequence cost of a","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"using ITensors\nusing Symbolics\n\nusing ITensors: contraction_cost\n\n@variables m, k, d\n\nl = Index(m, \"l\")\nr = Index(m, \"r\")\nh₁ = Index(k, \"h₁\")\nh₂ = Index(k, \"h₂\")\nh₃ = Index(k, \"h₃\")\ns₁ = Index(d, \"s₁\")\ns₂ = Index(d, \"s₂\")\n\nH₁ = ITensor(dag(s₁), s₁', dag(h₁), h₂)\nH₂ = ITensor(dag(s₂), s₂', dag(h₂), h₃)\nL = ITensor(dag(l), l', h₁)\nR = ITensor(dag(r), r', h₃)\nψ = ITensor(l, s₁, s₂, r)\n\nTN = [ψ, L, H₁, H₂, R]\nsequence1 = Any[2, Any[3, Any[4, Any[1, 5]]]]\nsequence2 = Any[Any[4, 5], Any[1, Any[2, 3]]]\ncost1 = contraction_cost(TN; sequence = sequence1)\ncost2 = contraction_cost(TN; sequence = sequence2)\n\nprintln(\"First sequence\")\ndisplay(sequence1)\ndisplay(cost1)\n@show sum(cost1)\n@show substitute(sum(cost1), Dict(d => 4))\n\nprintln(\"\\nSecond sequence\")\ndisplay(sequence2)\ndisplay(cost2)\n@show sum(cost2)\n@show substitute(sum(cost2), Dict(d => 4))","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"This example helps us learn that in the limit of large MPS bond dimension m, the first contraction sequence is faster, while in the limit of large MPO bond dimension k, the second sequence is faster. This has practical implications for writing an efficient DMRG algorithm in both limits, which we plan to incorporate into ITensors.jl.","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"Here is a more systematic example of searching through the parameter space to find optimal contraction sequences. Note, the TensorOperations.jl library must be loaded to use the optimalcontractionsequence function:","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"using ITensors\nusing Symbolics\n\nusing ITensors: contraction_cost, optimal_contraction_sequence\nusing TensorOperations: TensorOperations\n\nfunction tensor_network(; m, k, d)\n  l = Index(m, \"l\")\n  r = Index(m, \"r\")\n  h₁ = Index(k, \"h₁\")\n  h₂ = Index(k, \"h₂\")\n  h₃ = Index(k, \"h₃\")\n  s₁ = Index(d, \"s₁\")\n  s₂ = Index(d, \"s₂\")\n\n  ψ = ITensor(l, s₁, s₂, r)\n  L = ITensor(dag(l), l', h₁)\n  H₁ = ITensor(dag(s₁), s₁', dag(h₁), h₂)\n  H₂ = ITensor(dag(s₂), s₂', dag(h₂), h₃)\n  R = ITensor(dag(r), r', h₃)\n  return [ψ, L, H₁, H₂, R]\nend\n\nfunction main()\n  mrange = 50:10:80\n  krange = 50:10:80\n  sequence_costs = Matrix{Any}(undef, length(mrange), length(krange))\n  for iₘ in eachindex(mrange), iₖ in eachindex(krange)\n    m_val = mrange[iₘ]\n    k_val = krange[iₖ]\n    d_val = 4\n\n    TN = tensor_network(; m = m_val, k = k_val, d = d_val)\n    sequence = optimal_contraction_sequence(TN)\n    cost = contraction_cost(TN; sequence = sequence)\n\n    @variables m, k, d\n    TN_symbolic = tensor_network(; m = m, k = k, d = d)\n    cost_symbolic = contraction_cost(TN_symbolic; sequence = sequence)\n    sequence_cost = (dims = (m = m_val, k = k_val, d = d_val), sequence = sequence, cost = cost, symbolic_cost = cost_symbolic)\n    sequence_costs[iₘ, iₖ] = sequence_cost\n  end\n  return sequence_costs\nend\n\nsequence_costs = main()\n\n# Analyze the results.\nprintln(\"Index dimensions\")\ndisplay(getindex.(sequence_costs, :dims))\n\nprintln(\"\\nContraction sequences\")\ndisplay(getindex.(sequence_costs, :sequence))\n\nprintln(\"\\nSymbolic contraction cost with d = 4\")\n# Fix d to a certain value (such as 4 for a Hubbard site)\n@variables d\nvar_sub = Dict(d => 4)\ndisplay(substitute.(sum.(getindex.(sequence_costs, :symbolic_cost)), (var_sub,)))","category":"page"},{"location":"ContractionSequenceOptimization.html","page":"Contraction sequence optimization","title":"Contraction sequence optimization","text":"A future direction will be to allow optimizing over contraction sequences with the dimensions specified symbolically, so that the optimal sequence in limits of certain dimensions can be found. In addition, we plan to implement more algorithms that work for larger networks, as well as algorithms like[2] which take an optimal sequence for a closed network and generate optimal sequences for environments of each tensor in the network, which is helpful for computing gradients of tensor networks.","category":"page"}]
}
